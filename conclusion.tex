\section{Conclusion}

\subsection{Things we would have done differently}
\textbf{TODO}: change title of subsection

Encoder should also be trained using dropout as \textcite{multidigit_streetview}.

Retrain network from start for each experiment, however takes too much time.


\subsection{Our work compared to previous work}

% A great advantage of the

One of the greatest challenges in classifying year for a page is the sheer size of the page. Large images take a long time to process and they also contain a lot of other information. When recognizing house numbers from Google Street View, \textcite{multidigit_streetview} first made an automated system that cropped out the part of the image containing numbers as a fixed size $128 \times 128$ region. In our thesis, the attention model would be the corresponding mechanism for choosing a subregion in the image. However, the attention model requires the encoder CNN to run on the entire image, which increases running time.
% TODO: verify if attention model learned to recognize handwriting or not!
% In our model, the attention also learned to recognize other features than digits

Also in the case of \cite{FornesCnnCategorization}, they use word images as input.
\textbf{TODO}...

In contrast, \textcite{AttendAndTell} run on the entire input image while using attention to find the elements of interest. However, they do not transcribe any text but rather recognize objects in the image in order to generate new text. Hard attention, where the decoder only uses a single output unit from the encoder, worked rather well for object recognition but not so well for transcription. We conclude that a decoder can recognize objects without seeing them entirely, for example a leg might be enough to recognize a cat but a decoder can not guess the correct sequence of digits if it can only see the first two digits out of four.
% A key difference is that one output unit can recognize as an object without seeing it entirely, a piece of fur can be enough to recognize a cat while for transcription we need the network to consider the entire sequence.

Another major difference between our work and \cite{AttendAndTell} is that their decoder was sequential so for each iteration it looked at a new part of the image. So even though the decoder only looked at a single place in each iteration, over the output sequence most relevant parts of the image had been covered.


\subsection{Best future approach}

Thus it seems that attention mechanisms after running on the entire image is less suitable for information extraction and transcription than for object recognition and caption generation. Instead, it seems that the traditional approach of segmenting the page image to word images is still the best way to go. However, there are two major challenges with segmentation.

Firstly, the words have to be segmented correctly. This has proven to be a challenging problem in itself although the Esposalles dataset \cite{esposalles} can be very useful for training and evaluating segmentation models for population records.

Secondly, it is not clear how the transcription model could be trained using the indexed data. The problem lies in how to associate the extracted information with the segmented word images. It would likely take much manual work to create a trainable dataset, but there are two approaches to this. (1) the entire text could be transcribed exactly as in the case of \cite{esposalles}. Doing so would be useful if the final system has separate models for transcription and named entity recognition.
(2) each piece of extracted information could be associated with a word image. That would mean that some word images have no label and should be ignored although they may actually contain information.

Whichever approach is taken between exact transcription plus NER and information extraction, the end-to-end system must eventually learn which information is relevant and which is not.
For indexing purposes, we are only interested in the date, location, person, age and close relatives. Ignored information include baptism witnesses and summaries of life stories upon death.
% For example, baptism witnesses consist of many names but not much useful information for genealogical research.

% Since the indexed data is not a transcription of text but rather extracted information, several word images could contribute to a single piece of information.
