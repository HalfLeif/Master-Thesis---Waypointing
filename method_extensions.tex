\section{Extensions}

% As it turns out, the Swedish population dataset is quite challenging.
In this section we think about and discuss extensions to the model described above in order to fix various problems.
As we will see, not all such ideas will work; in fact, some of them can be discarded already in theory. Others, we evaluate empirically in section \ref{sec:experiments}.


\subsection{Alternative loss functions} \label{ssec:alt_loss}

Choosing a suitable loss function for training the model is paramount to successful learning.
Here we present and discuss some ideas for alternative loss functions for our task.
The two most promising alternatives, cartesian loss and multi-year loss, are evaluated in section \ref{sec:experiments}.
%The other presented alternatives we discard based on our discussion in this section.

% Here we discuss possible extensions and variations to the model described above.
% additional ideas that could be used to further improve our models.

\subsubsection{Disjoint labels}

% If we viewed each year as a disjoint label, with nothing in common

We could view each year as a disjoint label with nothing in common. In that case the decoder would need to have a readout layer of size $300$, one neuron for each possible year, instead of $3 \times 10$. Then we could simply take the cross-entropy loss on the readout layer directly.

A drawback of disjoint labels is that learning is not reused, training on an image with label $1772$ only benefits recognition of that label.
If we instead represent labels as a sequence of digits, the example of $1772$ helps in recognizing years that follow any of the patterns 17xx, 1x7x and 1xx2 where x refers to any digit.

Another drawback of disjoint labels is that there may be very few training examples for some years, as mentioned in section \ref{sssec:few_labels}.

% However, we wanted the model to learn to recognize individual digits and combine them into sequences.
% Assuming that digits look somewhat similar over different time periods, the learning rate should be higher for individual digits. For example if we are training on an image with the label $1772$, it should also benefit learning for all the years with patterns .

% Instead, we chose to recognize each digit independently.

\subsubsection{Cartesian loss} \label{sssec:cartesian}
%The other option is to take the Cartesian product of the three distributions into a single large distribution and take the cross-entropy of the combined distribution. A comparison between these two approaches is performed in section \ref{sssec:ind_digits}.

% A hybrid between disjoint labels and the three digit decoder
%

Here we introduce a loss function inspired by disjoint labels but lets the model reuse learning of individual digits. Above we suggested to sum the cross-entropy for each digit; instead we can compute the cartesian product of the readout layers to make a vector over the supported year range and take the cross-entropy directly on this vector. A comparison between the two loss functions is performed in section \ref{sssec:ind_digits}.


\subsubsection{Multi-year loss} \label{sssec:alt_multiyear}

It is not trivial to make a good loss function for multi-year labels. If we sum the loss for each year in the label, the images with many years have a higher influence than images with few years. Instead we can propose to take the average loss over the years but then another problem emerges: if for example the network would correctly classify an image as $1772$ where the label is $[1771, 1774]$, we still punish the network for not finding the other years.
In section \ref{ssec:result_multiyear} we compare using this multi-year loss against the default single-year loss.

\subsubsection{Distance loss}

Sometimes the network makes predictions that are very far away from the correct year, for example $1861$ instead of $1661$. Although we want to get the exact year, $1662$ could be considered a much better suggestion than $1861$. Cross-entropy penalizes each different digit but not the numeric distance, so $1662$ and $1861$ would get the same penalty, because in both cases, exactly one digit is wrong.

One intuitive loss for distance would be squared distance. For a single year label $y$, and a probability distribution $P(X=x)$ over the years we add a distance loss $L_D$ in addition to the cross-entropy:
\[
L_D(X, y) = \sum_{x \in \{1000, \ldots, 1999\}} (y-x)^2 P(X=x)
\]

This definition of a distance loss has the wanted property that predictions far away have a big loss while predictions close to the label only give a small loss. However, there are two fundamental problems with this choice of loss function: (1) bias and (2) independent digits.

For $L_D$ to be unbiased, the loss over a uniform distribution $U$ should be the same for all labels $y$ in the allowed interval. However, this is not the case. We can calculate the bias for different $y$ by summing $L_D$ over the allowed range. For simplicity, we map the integral year range $\{ 1000, \ldots, 1999 \}$ to a continuous range $[0,1]$ and integrate:
\[
L_D(U, y) = \int_0^1 (x-y)^2 P(U=x) dx = \frac{1}{3} + y(1-y)
\]

Thus we see that for the above definition of $L_D$, there is a strong bias for labels close to the center of the range. Likewise, a network trained with this distance loss becomes strongly biased to suggesting years closer to the middle of the output range.

% TODO mention in Experiments/Results? No.

The second problem with this distance loss is that even for a perfect network, there will be considerable predictions far away from the true label because we multiply the probability distributions for the digits independently. That is, if the network is $100\%$ certain that the last two digits should be $3, 4$ and the correct label is $1834$, we will still get strong signals for $1734$ and $1634$, although they have a great numeric distance to the correct label.
So even though it may make intuitive sense to penalize numeric distance, it is questionable whether it in the end would benefit learning.

\subsubsection{Proximity score}

Although the second problem would remain, the bias in the distance loss could be alleviated by using another approach:
%In order to solve the first problem we could suggest another approach:
instead of minimizing a distance loss, we can maximize a proximity score $S$ using a weight function $W$ around the label $y$:
\[
S(X, y) = \sum_x P(X=x) W(x, y)
\]

Because we want to reward predictions near $y$, we can use a Gaussian function as weight function:
\[
W(x, y) = \exp \left( \frac{-(y-x)^2}{\sigma^2} \right)
\]

We can then compute the bias in the same way as before, by integrating over a mapped interval $[0,1]$:
\[
S(U, y) = \int_0^1 P(U=x) W(x, y) dx =
\int_{-y}^{1-y} \exp \left( \frac{-x^2}{\sigma^2} \right) dx
%\frac{\sigma \sqrt{\pi}}{2} \left(
%\text{erf} \left( \frac{1-y}{\sigma} \right) +
%\text{erf} \left( \frac{y}{\sigma} \right)
%\right)
\]

We see that there is a bias against labels $y$ near the edges of the range, but for most values of $y$, the bias is negligible for small values of $\sigma$. The value of $\sigma$ could be chosen depending on the year interval in the label, so that a larger $\sigma$ is used for a larger interval.

A property of proximity score is to reward finding numbers numerically close to the label, such as $1669$ for $1670$. A potential problem with this is that the model is encouraged to recognize other features of $1670$ than the actual digits $\langle 6, 7, 0 \rangle$, such as page layout, handwriting style or image artifacts. Recognizing such additional features is a type of overfitting and do not generalize to other books or collections.
This kind of overfitting is another reason to not use loss functions based on numeric distance.

\subsection{Sequential information}

So far we have not utilized the information that pages in a book come in sequences, for example if the previous page was $1771$, the following page would have a strong probability of being $1771$ or $1772$.
In this section we suggest two ways of utilizing this contextual information for the purpose of improving the accuracy of the model when classifying new collections.

\subsubsection{Auxiliary input}

% This additional information could be taken into account by letting
% One approach would be to let the network have the prediction of the previous page as an auxiliary input to the decoder.
One approach would be to let the attention and decoder networks take one auxiliary input from the previous page, for example the decoder's hidden state or the digit predictions. This idea borrows from the RNN decoders by \textcite{machine_translation_attention} and \textcite{AttendAndTell} where the hidden state of the decoder RNN is passed to both the attention model and the RNN itself in the next iteration.

However, there are some problems with auxiliary input from the previous page. Firstly, it is not clear what the auxiliary input should be for the first page in a book but as in previous work we could use the zero vector. Secondly, during training we want to draw the images in a random order to prevent overfitting. However, we can not classify the previous image again because we would be missing the auxiliary input of that image. Instead, we could use the predictions from the previous epoch. Then the predictions used for auxiliary input would have been produced by presumably somewhat different weights than the current weights of the decoder. We hypothesize that this temporal mismatch between decoder weights would make learning from the auxiliary input slow and/or unstable.

If however the network could be trained using auxiliary input, it could increase the models predictive power.

\subsubsection{Post-processing} \label{sssec:alt_postproc}

Another approach to use the sequential information of books is to post-process the predictions. For example, if the network would suggest the sequence $\langle 1771, 1872, 1773 \rangle$, we could correct it to $\langle 1771, 1772, 1773 \rangle$.
Below we present a model for post-processing, which we evaluate in section \ref{sec:result_post_process}.
% post-processing using the \textbf{Viterbi algorithm} on the following model
% We suggest using  on the following model
% to attempt correcting the predictions.

For each image $t$, we define $z_{tj}$ to be the model's prediction probability that page $t$ has label $j$. We also introduce a jump distribution $Q(j, k)$ for making a transition from label $j$ to label $k$.

With this model, we consider each label to be a state and each book to be a sequence of state transitions. A transition at time $t$ from state $j$ to state $k$ is calculated as $z_{tk} Q(j, k)$. We can then apply the \textbf{Viterbi algorithm} using \textbf{Dynamic programming} to find the maximum probability sequence of state transitions.

We create a matrix $T$, so that that for each page $t$ and label $j$, $T[t,j]$ contains the maximum probability of a sequence so far. For the first page, we simply take the network predictions: $T[0,j] = z_{0j}$. For every other page, we use the following recurrence:
\[
T[t,j] = z_{tj} \max_k \left\{ T[t-1,k] Q(k, j) \right\}
\]

After computing the entire matrix $T$, the probability of the most likely sequence is the largest value in the last column of $T$. We can then reproduce the most likely sequence by backtracking.

To make the algorithm run faster, we only allow labels in the smaller interval 1600--1899 instead of the possible output 1000--1999.

\paragraph{Jump distribution}
The jump distribution for $Q(j, k)$ can be found by statistically counting the page transitions in the training data. We simplify the distribution by only taking into account the year difference between $j$ and $k$. In order to handle unseen transitions, we use \textbf{Laplace smoothing} with $\alpha=0.5$. We denote $N(r)$ as the count of year transitions with difference $r$, $N$ to be the total number of transitions and $K$ to be the number of allowed labels.

\[
Q(j, k) = Q(k-j) = \frac{N(k-j) + \alpha}{N + \alpha K}
\]

For images that have multiple years, we uniformly distribute the weight between the possible transitions of the two images. A transition from $[1771, 1772]$ to $[1772, 1773]$ would thus produce the following count: \{0: 0.25, 1: 0.5, 2: 0.25\}.

% TODO check new page
