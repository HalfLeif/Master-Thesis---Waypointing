
% Provide some background on neural networks

\chapter{Neural networks}
\label{sec:networks}
\textit{
In this chapter, we first give an introduction to neural networks and deep learning in the context of classification. We then describe convolutional neural networks and their application to real tasks in computer vision, in particular for digit recognition. At the end of the chapter we summarize some recent research on deep learning for computer vision that is not used in the thesis.
For further reading about neural networks and applied deep learning to various fields, we recommend the textbook by \textcite{GoodfellowBook}.
% For a more exhaustive reading of neural networks and deep learning we recommend the reader to search out a textbook in the field such as \cite{GoodfellowBook}.
}

\section{Introduction to neural networks}

Neural networks have recently gained popularity in various fields.
% They have been observed to be quite flexible in being able to classify data, find clusters and
% Neural networks can find meaningful information in a sea of data by clustering,
Applications include finding principal components, organizing meaningful clusters in a sea of data and generating new information, for example by translating natural languages \cite{machine_translation_attention}. Here, we will focus on the task of \textbf{classification}, which refers to choosing the correct class $y$ for some given input $\mathbf{x}$. For example to correctly recognize objects in an image or to select a strategic action to take in a game.

\subsection{Multilayer perceptron}

\input{figures/mlp.tex}

A \textbf{multilayer perceptron} (MLP) is organized in several \textbf{layers}, see illustration in figure \ref{fig:mlp}. Each layer consists of a number of \textbf{neurons}.
%, the first layer has the same number as elements in the input vector $\mathbf{x}$.
Each element in the input vector $\mathbf{x}$ becomes a neuron in the \textbf{input layer}. Each neuron $v_i$ in the following layers gets its value by computing a weighted sum over the neurons $u_j$ in the previous layer using weights  $w_{ij}$, a bias term $b_i$ and an \textbf{activation function} $g$:
\[
v_i = g\left( b_i + \sum_j w_{ij} u_j \right)
\]

In order for the network to utilize the power of several layers, the activation function must be non-linear. If $g$ would be linear, then the entire network would be a series of linear transformations which could be rewritten as a single linear transformation. Traditional choices of $g$ include the hyperbolic tangent and the logistic function (which is sometimes referred to as the sigmoid function).

The final layer is called the \textbf{readout layer} and has as many neurons as there are classes. The input $\mathbf{x}$ is classified as the class $y$ which has the greatest value in the readout layer. In order to estimate the certainty, we can compute pseudo-probabilities $p_i$ of the classes by taking the \textbf{softmax} of the readout values $o_i$:
\[
p_i = \frac{ \exp(o_i) }{ \sum_k \exp(o_k) }
\]

The layers between the input layer and the readout layer are referred to as \textbf{hidden layers}.

\subsection{Backpropagation} \label{sssec:BackProp}

In order to correctly classify new input, we need to \textbf{train} the network on some existing data set $D$ which consists of pairs of correct classifications $(\mathbf{x}^{(\mu)}, y^{(\mu)}$).
We let $\mathbf{y}^{(\mu)}$ denote a \textbf{one-hot} vector, that is all elements are zero except the indicated class whose associated element has the value one.
We then introduce a loss function $L$ to measure how different one network prediction $\mathbf{p}^{(\mu)}={\ldots, p_i^{(\mu)}, \ldots}$ is from its correct classification $\mathbf{y}^{(\mu)}$. Here we suggest the \textbf{squared error} loss function:
\[
L(\mu) = \frac{1}{2} \Vert
  \mathbf{y}^{(\mu)} - \mathbf{p}^{(\mu)}
\Vert ^2
\]

Given our loss function $L$, we can create a \textbf{cost} $H$ that represents the mean error over the data set $D$:
\[
H = \frac{1}{\vert D \vert} \sum_{\mu \in D} L(\mu)
\]

When $H$ is minimized, the network associates each input $\mathbf{x}^{(\mu)} \in D$ with the correct class. Since $H$ is differentiable, we can use standard \textbf{gradient descent} to optimize the network parameters, that is all weights $w_{ij}$ and biases $b_i$.
In iteration $k$ we use some \textbf{learning rate} $\eta$ and the gradient of the error $H$ to update the parameters $\mathbf{\theta}$:
\[
\mathbf{\theta}_{k+1} =
\mathbf{\theta}_k - \eta \nabla_{\mathbf{\theta}} H
\]

\subsection{Stochastic gradient descent}

Since gradient descent is susceptible to local minima, it is common to use other optimization techniques such as \textbf{stochastic gradient descent} (SGD).
In gradient descent above, we let $H$ sum over all data points in $D$ in every iteration. In contrast, SGD draws a new random subset $B \subset D$ in each iteration. We refer to $B$ as a \textbf{mini-batch}. We then compute the cost $H$ as a sum over $B$ and use its gradient to update the network parameters:
\[
H = \frac{1}{\vert B \vert} \sum_{\mu \in B} L(\mu)
\]

\[
\mathbf{\theta}_{k+1} =
\mathbf{\theta}_k - \eta \nabla_{\mathbf{\theta}} H(k)
\]

It is common to randomly partition $D$ into mini-batches so that all mini-batches are disjoint. Iterating over the entire dataset is then referred to as an \textbf{epoch}.
The random partition of $D$ should be different for each new epoch.
To achieve the wanted accuracy, it is often necessary to train over many epochs.


\subsection{Testing and evaluating}

Because neural networks have many parameters, they risk memorizing the training data and hence fail to learn the general patterns that we actually want them to recognize.
% \cite{AlexNet, FornesCnnCategorization}.
This phenomenon is knows as \textbf{overfitting} and can constitute a large problem for small datasets. In order to estimate overfitting, we divide the original dataset into one part for training and one for testing. The training data is used for backpropagation while the test data is exclusively used for testing. If the network begins to overfit on the training data, the accuracy on the test data decreases and the training should be aborted.

By stopping the training when overfitting on the test data occurs, it is possible to indirectly overfit the test data. Therefore, one can instead divide the original dataset into three disjoint sets and use the third part to evaluate the final accuracy.


\section{Methods for deep neural networks}
% Very powerful, see for example AlphaGo
% Many parameters, needs lot of data

One of the reasons why neural networks have recently achieved state-of-the-art in so many tasks is the use of very deep networks, that is networks with many layers.
% TODO add citation
The more layers, the more complex tasks can be learned. To the surprise of many, computers recently even outperformed humans in the complex game of Go \cite{AlphaGo, AlphaGoTuringTest}.

The large number of parameters in deep networks is both a strength and a weakness since it requires a large amount of training data as well as considerable computation power. More parameters can also memorize more and thus make the network more prone to overfitting \cite{AlexNet}.

\subsection{Dropout}

One way to reduce the risk of overfitting is by applying \textbf{dropout}  \cite{Dropout, AlexNet, FornesCnnCategorization}.
%Another problem with large networks is overfitting which means that the network learns the training set by heart instead of learning to recognize the general patterns \cite{AlexNet, FornesCnnCategorization}.
The dropout method introduces a probability of setting each neuron's activation to $0$ during training, often with $50\%$ probability. This forces the network to learn multiple paths for recognizing different features and hence creating a more robust representation. Dropout also reduces the computational cost because fewer neurons participate in each training step.
Note that dropout is not necessarily done for every layer in the network.


\subsection{Weight decay}

\textbf{Weight decay} \cite{WeightDecay} is another common technique to avoid overfitting cite[Chapter~7]{GoodfellowBook}. We add a new penalty term for weight regularization to our previous cost $H$, where $\mathbf{w}$ denotes all weight parameters $w_{ij}$ but not the bias parameters $b_i$:
\[
\tilde{H} = H + \alpha \Omega(\mathbf{w})
\]

The amount of regularization is determined by the hyper-parameter $\alpha \in [0,\infty)$.
A common version of weight decay is \textbf{L2 regularization}:
\[
\Omega(\mathbf{w}) = \frac{1}{2} \mathbf{w}^T \mathbf{w}
\]

By computing the gradient for the new cost metric $\tilde{H}$, we get the new update rule for the weights:
\[
\mathbf{w}_{k+1} =
\mathbf{w}_k - \eta \nabla_{\mathbf{\theta}} \tilde{H}(k) =
(1 - \eta \alpha) \mathbf{w}_k - \eta \nabla_{\mathbf{\theta}} H(k)
\]

We see that $\mathbf{w}$ decays with a factor $(1 - \eta \alpha)$ in each iteration, so the weights decay exponentially over time. Thus the weight decay makes sure that the weights are kept small no matter for how long the network is trained; that is, no weight parameter can grow indefinitely. Empirically, it has been observed that networks can be trained longer without overfitting when the weights are bounded \cite[Chapter~3]{NielsenBook}.

Since the penalty term does not include the bias terms, the update rule for the bias parameters is unchanged.


\subsection{Cross-entropy loss} \label{sssec:CrossEntropy}

Above in section \ref{sssec:BackProp}, we used squared error as the loss function $L$. One of the problems with squared error is that a large error does not always result in a large gradient \cite[Chapter~3]{NielsenBook}.
A more commonly used loss function is \textbf{cross-entropy}, which has the property that its gradient depends directly on the distance between the network output and the correct value.
Thus in most cases, cross-entropy is a better choice as loss function than squared error.

For training example $\mu \in D$, we denote the network softmax output in neuron $i$ as $p_i^{(\mu)} \in (0,1)$, while $y_i^{(\mu)}$ takes the value one if $i$ is the correct class and the value zero otherwise. Then we can define the cross-entropy loss $L$ as:
\[
L(\mu) = - \sum_i y_i^{(\mu)} \log p_i^{(\mu)}
\]

% For the case of two classes, that is a single output neuron $p$, the expression can be simplified as:
%
% \[
% L(\mu) = - \left(
%   y^{(\mu)} \log p^{(\mu)} + (1 - y^{(\mu)}) \log (1 - p^{(\mu)})
% \right)
% \]

This expression may not intuitively make sense as a loss function but we can make the following observations:

\begin{itemize}
    % \item Because $p_i \in (0, 1)$, both $\log p_i$ and $\log (1 - p_i)$ must be negative and thus each term in the sum is negative. We have a minus sign before the sum, so $L(\mu)$ must be positive.
    % \item The correct label $y_i$ can only take the value $0$ or $1$. If $y_i=1$, then the expression can be simplified as $\log p_i$ which is greater the farther $p_i$ is from the desired value $1$. By symmetry, the same holds for when $y_i = 0$.
    \item The label $y_i$ only takes the value $1$ for the correct class $i$; for all other classes it takes the value $0$. Thus, our sum contains only one non-zero term: $\log p_i^{(\mu)}$.
    \item Because $p_i \in (0, 1)$, $- \log p_i$ must be positive and hence $L(\mu)$ must also be positive for any training example $\mu$.
    \item The closer $p_i$ gets to $100\%$ certainty of the correct class $i$, the closer $\log p_i$ gets to zero and the smaller the loss becomes.
    %Since $- \log p_i$ gets smaller the closer $p_i$ is to the optimal value $$
    \item Although the sum only has one non-zero term, the loss gets propagated through all output neurons through the application of softmax to calculate $p_i$. The smaller $p_i$ is, the more weight comes from other output neurons, which consequently get penalized by the loss.
\end{itemize}

% In conclusion we can observe that $L(\mu) \geq 0$ and is minimized when the $y_i$ predict the correct label. Furthermore, the amplitude of the loss depends directly on how far away the actual output $p_i$ is from the expected output $y_i$. Therefore, cross-entropy has the properties of a good loss function.

By these observations, we can see that minimizing $L(\mu)$, which is always positive, will train the network to increase the probability $p_i$ for the correct label $i$.
%improve the probability for the network predicting the correct label.

\subsection{Momentum}

One problem with stochastic gradient descent (SGD) is that the learning can zig-zag across valleys in the optimization plane, especially for noisy gradients \cite[Chapter~8]{GoodfellowBook}. Another problem is that the learning becomes very slow for stable but small gradients. A suggested solutions to both problems is the use of \textbf{momentum}. When training with momentum, instead of directly updating the parameters $\mathbf{\theta}$, we apply the gradients to a decaying momentum $\mathbf{v}$:
\[
\mathbf{v}_{k+1} = \alpha \mathbf{v}_k - \eta \nabla_{\mathbf{\theta}} H(k)
\]

The accumulated momentum is then applied to update the parameters:
\[
\mathbf{\theta}_{k+1} = \mathbf{\theta}_k + \mathbf{v}_{k+1}
\]

With momentum, the learning no longer zig-zags across valleys because the accumulated momentum would point directly in the direction of the valley. For stable but small gradients, the learning would accelerate because the accumulated momentum grows in the same direction. Thus both problems presented above are mitigated by momentum.

\subsection{Alternative learning algorithms}

Although SGD is a very common technique to minimize the cost $H$, there are several popular alternatives. In stochastic gradient descent, the learning rate is a fixed hyper-parameter $\eta$ that is set manually by the programmer and is often adjusted after considerable empirical experimentation. A too high learning rate may overshoot the optimal values and hinder convergence while a too small learning rate may unnecessarily cause a long training time.

For many tasks it is desirable that the learning rate is high in the beginning when the network is very wrong while the learning rate should be small at the end when the distance to the optimal solution is small. There are several popular methods that use an adaptive learning rate: \textbf{AdaGrad}, \textbf{RMSProp} and \textbf{Adam} \cite[Chapter~8.5]{GoodfellowBook}. However, there is no clear consensus about which method is the best. People seem to prefer the method they are used to since their personal experience helps with setting the hyper-parameters.

Although gradient-based methods dominate the field, it is not the only possible approach. \textbf{Particle swarm optimization} has also been suggested for training neural networks in cases when it may be difficult to compute the gradient \cite[Chapter~5]{StochasticOpt}.

\subsection{Rectified linear activation}

Very deep networks suffer from the \textbf{vanishing gradient} problem.
Backpropagation updates each parameter according to its gradient but if the gradient is very small, it will take a long time before the network converges.
Such small gradients can occur when using the traditional activation functions, hyperbolic tangent and the logistic function, because they have very small gradients for large activations \cite{AlexNet}.
This can largely be solved by using the \textbf{rectifier function} (ReLU) for activation: $g(x) = \max \{0, x\}$, because the derivative is the same for very large activations as for small activations of the same sign.


\subsection{Pre-training and transfer learning}

When the intended task has limited training data or takes a long time to train, it can be useful to start with a network that has already been trained on a different but related task or data set.
For example, \cite{SpatialTransformerNetworks} use the Inception network \textbf{pre-trained} on ImageNet.
The idea is that the parameters in a network trained on a related task is probably closer to their optimal value than randomly initialized parameters. For example, recognizing edges and corners are central to many visual recognition tasks.

Pre-training, especially \textbf{greedy supervised pre-training}, can also refer to training a network a few layers at a time while fixating the other layers. In the rest of the thesis, we refer to pre-training as training the same model on a different dataset as described above.


\subsection{Confidence thresholding}

% TODO: why cited as I. J. Goodfellow and not just Goodfellow? Fix...

Certain tasks, like automatic house number transcriptions in Google Street View, require the model to have a very high accuracy to be useful \cite{multidigit_streetview}.
For these instances, the model should output a \textbf{confidence} for its classification. If the confidence is higher than some pre-determined threshold $\tau$, we keep the prediction; otherwise we ignore the prediction and conclude that the model is not sure about that example.
% In order to increase the accuracy, the predictions that have a confidence lower than some threshold $\tau$ may be discarded.
We then need to use two metrics for evaluating the model's performance: \textbf{precision} and \textbf{recall}.

The precision measures the quality of the predictions that were higher than the threshold. It is the accuracy, or percentage of correct predictions, among the predictions that were not discarded.
% Out of the non-discarded predictions, the precision is the percentage of correct predictions.

Recall on the other hand measures how many correct predictions were discarded due to low confidence. Instead of measuring recall, we can use \textbf{coverage}, which is simply the percentage of predictions whose confidence was above the threshold $\tau$.
% TODO: is it really?
% More precisely, recall is the number of correct predictions that were not discarded divided by the total number of correct predictions.

In order to compare models, we can then plot precision vs recall or coverage for different values of $\tau$. For $\tau=0$, the precision is the same as accuracy and the coverage is $100\%$. For $\tau=1.0$, everything is discarded so the precision is $100\%$ but the coverage is $0\%$.

\section{Deep learning in computer vision}

Both machine translation and image captioning used to be solved by solving each sub-problem separately but this kind of approach has been outperformed by end-to-end systems using deep learning \cite{machine_translation_attention, ShowAndTell}. One neural network \textbf{encoded} the input to a fixed-length vector representation which was \textbf{decoded} by another network. In the case of machine translation both the encoder and decoder was implemented by a recursive neural network (RNN), which was a good choice since RNNs are highly suitable for processing sequences such as sentences. The image captioning system was implemented in a similar manner but using a \textbf{convolutional neural network} (CNN) as encoder. We delve deeper into the workings of CNNs in the next section.


\section{Convolutional neural networks}

Since the publication of the AlexNet in 2012 \cite{AlexNet}, CNNs have gained large attention in computer vision for achieving state-of-the-art in various tasks such as object-detection, segmentation, video classification and object tracking \cite{InceptionV3}.

% TODO add citations

Each layer in a CNN consists of a two-dimensional grid of vectors, which we will refer to as \textbf{units}. Throughout the network, each unit will correspond to different regions in the input image. Sometimes we will refer to this grid as a three-dimensional tensor where the depth is the size of the vectors. The \textbf{representational size} of a layer is simply the product of its height, width and depth.

% The \textbf{representational size} of a layer is the width times the height of the activation map times the number of filters. In short, it is the number of scalars in the tensor currently representing the input image.

% The depth of a CNN refers to the number of layers while the width of a CNN refers to the depths of its layers.

Similar to MPLs, the input layer has the same width and height as the input image while the depth of the input layer matches the number of color channels in the image (one for gray scale, three for RGB and four for RGBA).

% In CNNs, each layer has a width, height and depth. Like in MLPs, the input layer has the width and height of the input image while the depth correspond to the number of color channels (one for gray scale, three for RGB and four for RGBA).
% We see each layer as a two-dimensional grid of vectors, where each vector is referred to as a \textbf{unit}.
%We now refer to each number in the network as a \textbf{unit} instead of a neuron.

\subsection{Convolutions}

\input{figures/filter.tex}

As indicated by the name, CNNs mainly consist of \textbf{convolutional layers}. Like all other layers in a CNN, a convolutional layer is three-dimensional and the depth is determined by how many filters it applies to the previous layer.  Each filter produces a two-dimensional \textbf{activation map}, as illustrated in figure \ref{fig:filter}. The values in a convolutional layer is simply the activation maps stacked on top of each other, which forms a three-dimensional structure that is passed on to the next layer.

Each scalar in the activation map comes from a weighted sum over a three-dimensional region in the previous layer over its full depth. For example a $3 \times 3$ filter applied to a layer of depth 32 has $3 \cdot 3 \cdot 32=288$ weights and one bias term.
As in MLPs, the sum is passed to to a non-linear activation function such as $\tanh$.

The $3 \times 3$ regions for adjacent units are usually overlapping.
By adjusting the so called \textbf{stride} one can produce fewer output units in the activation map by ignoring some locations in between.
% It also possible to explicitly reduce or avoid such overlapping by setting a \textbf{stride} greater than one.
Thus the representational sizes of the following layers are reduced.
However, in our thesis all CNNs use the default stride, so no location is ignored.

% The activation maps for all filters are stacked together to form a three-dimensional output, which is passed to the next layer.

Because a filter uses the same weights for all sub-regions in the previous layer, the total number of parameters are dramatically reduced compared with a fully connected layer as in an MLP. Moreover, because the same filter is applied to every sub-region, the filter can detect objects regardless of their position in the image, increasing the predictive power of the network.
It also means that the same filter can be trained by images whose objects appear in different positions, which reduces the training time and makes more use of the available training data.
% Most importantly, reuse of weights increases the ability to generalize.
%it does not matter where in the input image the object is located.

\subsection{Pooling}
\input{figures/maxpool.tex}

Besides filters, CNNs also use \textbf{pooling layers}, which compress the signals of the previous layer into a smaller representation without using any additional parameters.
%in order to increase the receptive field without increasing the number of parameters in the model.
% Separately for each activation map,
Pooling aggregates a two-dimensional region of units (e.g. $2 \times 2$) into a single unit, typically by taking the maximum value element-wise. A maxpool operation is illustrated in figure \ref{fig:maxpool}.
The regions are non-overlapping so a $2 \times 2$ pooling layer divides the representational size by 4.
Since the representation is now smaller, the remaining computations are cheaper to perform, resulting in reduced memory usage and running time.

\subsection{Receptive field}
Since both convolutional layers and pooling layers gathers information from a larger field, units in later layers in the CNN receive information from a larger \textbf{receptive field} in the input image.
% Each $2 \times 2$ pooling layer doubles the receptive field in each dimension, while every $3 \times 3$ convolutional layer increases the receptive field with 2.
Therefore, the filters in early layers typically learn to detect low-level features like edges and curves while filters in later layers can detect more complex objects like cats or faces.
By synthesizing the input image to maximize a specific filter's activation, it is possible to get an idea of what the filter has learned to recognize \cite{VisualizeCnn}.

We can calculate the size of the receptive field for each output unit by doubling the field for each $2 \times 2$ pooling layer and adding $n-1$ for each $n \times n$ convolutional layer.
If we have a network that consists of two $5 \times 5$ convolutional layers with a single $2 \times 2$ max-pool layer in between, then we can compute the pixel size $r \times r$ of the receptive field as follows:
\[
r = 4 + 2(4 + 1) = 14
\]

\subsection{Output}
At the end of the network, there are often a few fully connected layers (MLP) where the last layer represent the output of the network, for example pseudo-probabilities for different categories using softmax like discussed previously.

\subsection{Network architecture for faster computation}

The performance of CNNs typically increases with greater width and depth, but it comes at a higher computational cost \cite{InceptionV3}. However, by choosing a good network architecture the same performance can be achieved at a substantially lower computational cost. One such guideline is to refactor large filters ($5 \times 5$, $7 \times 7$) into several small consecutive filters ($3 \times 3$). \textcite{InceptionV3} also suggest using more pooling layers early in the network as well as maintaining a balance between depth and width of the network. The representational size should also decrease slowly throughout the network without

% \textcite{InceptionV3} recommend that representational bottlenecks are avoided and that the representational size should decrease slowly after each layer.

\subsection{Zero padding}

Applying a $3 \times 3$ filter to an input image of size 10x10 will output an image of size 8x8 since there are only 8 positions in a single row where the filter can fit. For deep networks, this shrinking might cause the final representation to become too narrow.
One can pad each activation map with zeroes to instead maintain the same size, which solves the problem at little additional computional cost.

\section{Attention models}
\label{ssec:attention}
% Since waypointing only needs high-level information in a limited domain, complete transcription is unnecessary. Instead we can use recent advances in image classification.

% TODO write this part again in more detail, especially after spending more time with the literature here.

In contrast to CNNs, the human eye does not process the entire scene with equal precision but focuses on the most relevant parts \cite{DeepMindAttention}.
Similarly, we can let the system pay \textbf{attention} to a small part of the image at a time and successively increase the system's understanding of the image.
By choosing good locations for our attention, the important parts of the image are captured while the irrelevant parts are ignored.
This reduces the computation time for training and also increases the precision of the network.
% The process of how to select a location to focus on is called an \textbf{attention model} and is often implemented using a \textbf{recurrent neural network} (RNN).
Besides image classification, attention models have also been highly successful for image captioning \cite{AttendAndTell} and machine translation \cite{machine_translation_attention}.

% \subsection{Details}
We now describe how the attention model that was introduced by \textcite{machine_translation_attention} works in more detail.
The goal of the attention model is to produce a vector $\mathbf{z} \in \mathbb{R}^D$ that represents the entire input image by weighing different parts of the output from the encoder CNN.

The encoder output is three-dimensional but can be seen as a grid of $L$ feature vectors. Each feature vector $\mathbf{a_i} \in \mathbb{R}^D$ corresponds to a specific location in the input image. We feed each $\mathbf{a_i}$ to an MLP which produces a scalar $e_i \in \mathbb{R}$. In the paper, the decoder uses an RNN whose hidden state is also used as input to the MLP besides the feature vector $\mathbf{a_i}$.
The scalar attention weight $\alpha_i$ for each location $i$ can then be computed by taking the softmax of $e_i$ over all locations:
\[
\alpha_i = \frac{ \exp(e_i) }{ \sum_{k=1}^L \exp(e_i) }
\]

\subsection{Soft attention} \label{sssec:soft_attention}
By definition, the attention weights sum to one. A \textbf{soft attention} model computes the representative vector $\mathbf{z}$ by summing over the feature vectors $\mathbf{a_i}$, using attention as weights:
\[
\mathbf{z} = \sum_{k=1}^L \alpha_i \mathbf{a_i}
\]

\subsection{Hard attention}
A \textbf{hard attention} model instead interprets the ${\alpha_i}$ as a probability distribution over locations $i$. By sampling this distribution, $\mathbf{z}$ is chosen to be the sampled unit $\mathbf{a_i}$, ignoring the rest of the image.


\section{CNNs in multi-digit recognition}

CNNs have not only been successful at image classification in general, but also specifically at detecting digits in images.

\subsection{Zip code detection}

Digit recognition has been studied extensively, especially for recognition of zip-codes in the US postal service \cite{lecun_1989, lecun_1990}. Originally, the digits were segmented manually and linearly transformed to a fix input size of 16x16 pixels.
Each digit image was then classified using a CNN with 3 or 4 hidden layers, with $2 \times 2$ pooling between each convolutional layer.

A complete zip-code recognition system locates the place of the zip-code and segments the zip-code into digit images \cite{zipcode_system}. The segmentation is performed by finding \textbf{connected components} (CC) in the binarized image. We discuss more about connected components and segmentation in section \ref{sec:alt_htr}.
% After binarizing the image (explained in section \ref{sssec:binarization}), a connected component.
If the CC has a high confidence in classification it is removed, otherwise it is either split or combined with adjacent CCs until a segmentation has been found where each classification has a high enough confidence. This is done by building a directed acyclic graph where each proposed segment is a node. The length of a path is defined as the product of the classification confidence for each node in the path. The best segmentation is then the path of greatest length in the graph. In order to avoid redundant computations, the segmentation can be done indirectly after the convolutional layers instead of on the input image \cite{lecun_multidigit}.

\subsection{Street view house numbers}

% TODO merge places about SVHN...

More recently, deep neural networks have achieved state-of-the-art in multi-digit recognition in street view images \cite{multidigit_streetview}. Instead of handling localization, segmentation and classification as separate tasks, they solve all of them using a single CNN with 11 layers. The output is modeled as a sequence $S$ of digits with a maximum length $N$. The sequence $S$ consists of a random variable $L=n$ for the length of the sequence and $n$ variables $S_i$, one for each digit. In order to handle images without numbers $L$ is allowed to have the value zero and in order to handle longer sequences than $N$, $L$ has a special value "greater than $N$". For an input image $X$, the system can be trained by maximizing $\log P(S \vert X)$. Classification is similarly done by $\text{argmax}_S P(S \vert X)$.
Because each variable has a very small domain, they can be implemented with independent softmax outputs.
Although this model works well for digit recognition of short sequences $N \leq 5$ as well as \textbf{optical character recognition} (OCR) in CAPTCHAs, the authors speculate that the method is not suitable for long or unbounded sequences.

Another suggested approach to digit recognition in street view is to combine CNNs with \textbf{hidden markov models} (HMMs) \cite{multidigit_streetview_CNN_HMM}. They use a sliding window to extract multiple overlapping frames on the input image, classify each frame using a CNN to either a digit or null and then feed the sequence of frame labels to an HMM.


\section{Additional topics}
Here we present some related recent progress in the field of neural networks in computer vision, although we do not directly use these results in the thesis.
Evaluation of these methods on our task is considered future work.

\subsection{Batch normalization}

An alternative to dropout was suggested by \textcite{BatchNormalization}. Besides reducing the risk for overfitting, the authors claim that it allows for a higher learning rate in deep networks and thus faster convergence.

For some network layer $\mathbf{x} = {x_1, \ldots, x_d}$, we normalize each $x_i$ to zero mean and unit variance over the current mini-batch:
\[
\hat{x_i} = \frac{x_i - E[x_i]}{ \sqrt{\text{Var} [x_i]} }
\]

We then use two learned parameters $\gamma$ and $\beta$ to instead output $z_i$:
\[
z_i = f_{BN}(x_i; \gamma, \beta) = \gamma \hat{x_i} + \beta
\]

The batch normalization can either be applied before or after the activation function. The authors suggest that the best result is achieved by applying the batch normalization before the non-liner activation $g$ but after the weights $w_{ij}$ and bias $b_i$:
\[
v_i = g\left( f_{BN}\left( b_i + \sum_j w_{ij} u_j; \gamma, \beta \right) \right)
\]

\subsection{Visual-semantic alignment}

% Another way to model regions in the input is by using a R-CNN which have also been very successful at image captioning

Although \textcite{AttendAndTell} successfully used attention models for image captioning,
an alternative successful method instead uses a \textbf{region convolutional neural network} (R-CNN) as encoder \cite{VisualSemanticAlignment}.
The original R-CNN method works by proposing 2000 regions in the image that are the most likely to contain an object \cite{RCNN}. Each region is then fed to a regular CNN for encoding before classification.

Since many regions overlap, it is unnecessary to compute the convolutions for them independently \cite{FastRCNN}. Instead the entire input image is processed through the CNN once and the feature vector of each region is extracted from the resulting activation map. After this improvement, the bottleneck is selecting the regions \cite{FasterRCNN}. By making a neural network to select regions from the activation map instead of from the image, the computation speed can be significantly reduced. However, this requires the training data to contain bounding boxes for the objects to detect.

For the case of image captioning, the correspondences between words and regions in the image is unknown so this relationship is modeled by latent variables \cite{VisualSemanticAlignment}.

\subsection{Spatial transformer networks}

Another exciting innovation for CNNs is \textbf{spatial transformer modules} \cite{SpatialTransformerNetworks}. Each spatial transformer module dynamically finds an interesting part of the input image and transforms it by spatial operations such as scaling, cropping and rotation. For example in handwriting transcription, such a module could in theory locate and normalize words in the input image. Spatial transformer networks has been proven to work well for both digit recognition and image classification.

\subsection{Spatial pyramid pooling}

Convolutional layers and pooling layers can handle input images of changing sizes since the filters just slide over the entire image, so the size of the output matches the input.
However, because the last network layers are fully connected, they can only handle input of a fixed size. A common solution to this is to rescale or crop the input \cite{FornesCnnCategorization}. If the input consists of segmented images of handwritten words, then cropping will cut away critical information, while rescaling would risk distorting the handwriting beyond recognition. A proposed solution to this is using a layer of \textbf{spatial pyramid pooling} (SPP) \cite{FornesCnnCategorization}.

% \subsection{Residual networks}

% TODO cite Microsoft ResNet for architecture of Residual modules. There are also residual networks of residual networks.
