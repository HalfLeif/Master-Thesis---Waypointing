
% Provide some background on neural networks

\section{Neural networks}
\label{sec:networks}
In this section, we give a brief introduction to how neural networks function and describe recent research in applying neural networks to tasks in computer vision.
For a more detailed description of neural networks and deep learning we refer the reader to a textbook in the field such as \cite{GoodfellowBook}.

\subsection{Introduction to neural networks}

Neural networks have recently become increasingly popular in many diverse fields.
% They have been observed to be quite flexible in being able to classify data, find clusters and
% Neural networks can find meaningful information in a sea of data by clustering,
Neural networks can be utilized to find the top principal components and find meaningful clusters in a sea of data. They can also be used to generate new information, for example in machine translation \cite{machine_translation_attention}. Here, we will focus on the task of \textbf{classification} which refers to choosing the correct class $y$ for some given input $\mathbf{x}$. This can be to correctly recognize objects in an image or select a strategic action to take in a game.

\subsubsection{Multilayer perceptron}

\input{figures/mlp.tex}

A \textbf{multilayer perceptron} (MLP) is organized in several \textbf{layers}, see illustration in Figure \ref{fig:mlp}. Each layer consists of a number of \textbf{neurons}.
%, the first layer has the same number as elements in the input vector $\mathbf{x}$.
Each element in the input vector $\mathbf{x}$ becomes a neuron in the input layer. Each neuron $v_i$ in the following layers gets its value by computing a weighted sum over the neurons $u_j$ in the previous layer using weights  $w_{ij}$, a bias term $b_i$ and an \textbf{activation function} $g$:

\[
v_i = g\left( b_i + \sum_j w_{ij} u_j \right)
\]

In order for the network to utilize the power of several layers, the activation function must be non-linear. If $g$ would be linear, then the entire network would be a series of linear transformations which could be rewritten as a single linear transformation. Traditional choices of $g$ include the hyperbolic tangent and the logistic function (which is sometimes referred to as the sigmoid function).

The final layer is called the \textbf{readout layer} and has as many neurons as there are classes. The input $\mathbf{x}$ is classified as the class $y$ which has the greatest value in the readout layer. In order to estimate the certainty, we can compute pseudo-probabilities $p_i$ of the classes by taking the \textbf{softmax} of the readout values $o_i$:

\[
p_i = \frac{ \exp(o_i) }{ \sum_k \exp(o_k) }
\]

\subsubsection{Backpropagation}

In order to correctly classify new input, we need to \textbf{train} the network on some existing data set $D$ which consists of pairs of correct classifications $(\mathbf{x}^{(\mu)}, y^{(\mu)}$).
We let $\mathbf{y}^{(\mu)}$ denote a \textbf{onehot} vector, that is a all elements are zero except the indicated class whose associated element has the value $1$.
We then introduce a loss function $L$ to measure how different one network prediction $\mathbf{p}^{(\mu)}$ is from its correct classification $\mathbf{y}^{(\mu)}$:
% For each example $\mu \in D$, we
% We introduce a metric $H$ for how different the network pseudo-probabilities $\mathbf{p}^{(\mu)}$ are from the correct classification $\mathbf{y}^{(\mu)}$.

\[
L(\mu) = \Vert
  \mathbf{y}^{(\mu)} - \mathbf{p}^{(\mu)}
\Vert ^2
\]

Given our loss function $L$, we can create a metric $H$ that represents the mean error over the data set $D$:

\[
H = \frac{1}{2 \vert D \vert} \sum_{\mu \in D} L(\mu)
\]

When $H$ is minimized, the network associates each input $\mathbf{x}^{(\mu)} \in D$ with the correct class. Since $H$ is differentiable, we can use standard \textbf{gradient descent} to optimize the network parameters, that is all weights $w_{ij}$ and biases $b_i$.

In iteration $k$ we can thus use some \textbf{learning rate} $\gamma$ and the gradient of the error $H$ to update the parameters $\mathbf{\theta}$:

\[
\mathbf{\theta}_{k+1} \leftarrow
\mathbf{\theta}_k - \gamma \nabla_{\mathbf{\theta}} H
\]

\subsubsection{Stochastic gradient descent}

Since gradient descent is susceptible to local minima, it is common to use other optimization techniques such as \textbf{stochastic gradient descent} (SGD).
In gradient descent above, we let $H$ sum over all data points in $D$ in every iteration. In contrast, SGD draws a new random subset $B \subset D$ in each iteration. We then compute $\hat{H}$ as a sum over $B$ and use its gradient to update the network parameters:

\[
\hat{H} = \frac{1}{2 \vert B \vert} \sum_{\mu \in B} L(\mu)
\]

\[
\mathbf{\theta}_{k+1} \leftarrow
\mathbf{\theta}_k - \gamma \nabla_{\mathbf{\theta}} \hat{H}(k)
\]

It is common to randomly partition $D$ into mini-batches so that all mini-batches are disjoint. Iterating over the entire dataset is then referred to as an \textbf{epoch}.
To achieve the wanted accuracy, it is often necessary to train over many such epochs.
The random partition of $D$ should be different for each new epoch.

\subsubsection{Testing}

Because neural networks have many parameters, they risk memorizing the training data and hence fail to learn the general patterns that we actually want them to recognize.
% \cite{AlexNet, FornesCnnCategorization}.
This phenomenon is knows as \textbf{overfitting} and can constitute a large problem for small data sets. In order to estimate overfitting, we divide the original data set into one part for training and one for testing. The training data is used for backpropagation while the test data is exclusively used for evaluation. If the network begins to overfit on the training data, the accuracy on the test data decreases.


\subsection{Deep neural networks}
% Very powerful, see for example AlphaGo
% Many parameters, needs lot of data

One of the reasons why neural networks have recently achieved state-of-the-art in so many tasks is the use of very deep networks, that is networks with many layers.
% TODO add citation
The more layers, the more complex tasks can be learned. To the surprise of many, computers recently even outperformed humans in the complex game of Go \cite{AlphaGo, AlphaGoTuringTest}.

The large number of parameters in deep networks is both a strength and a weakness since it requires a large amount of training data as well as considerable computation power. More parameters can also memorize more and thus makes the network more prone to overfitting \cite{AlexNet}.

\subsubsection{Pre-training}

% TODO discuss what and why

\subsubsection{Dropout}

One way to reduce the risk of overfitting is by applying \textbf{dropout}  \cite{AlexNet, FornesCnnCategorization}.
%Another problem with large networks is overfitting which means that the network learns the training set by heart instead of learning to recognize the general patterns \cite{AlexNet, FornesCnnCategorization}.
The dropout method introduces a probability of setting each neuron's activation to $0$ during training, often with $50\%$ probability. This forces the network to learn multiple paths for recognizing different features and hence creating a more robust representation. Dropout also reduces the computational cost because fewer neurons participate in each training step.

\subsubsection{Rectified linear activation}

Very deep networks suffer from the \textbf{vanishing gradient} problem.
Backpropagation updates each parameter according to its gradient but if the gradient is very small, it will take a long time before the network converges.
Such small gradients can occur when using the traditional activation functions, hyperbolic tangent and the logistic function, because they have very small gradients for large activations \cite{AlexNet}.
This can largely be solved by using the \textbf{rectified linear} (ReLU) function for activation: $g(x) = \max{0, x}$.

\subsubsection{Faster gradient descent}
\textbf{TODO}: briefly describe Adam optimizer and using cross-entropy instead of squared error.

\subsection{Deep learning in computer vision}

Both machine translation and image captioning used to be solved by solving each subproblem separately but this kind of approach has been outperformed by end-to-end systems using deep learning \cite{ShowAndTell}. One neural network \textbf{encoded} the input to a fixed-length vector representation which was \textbf{decoded} by another network. In the case of machine translation both the encoder and decoder was implemented by an RNN which are highly suitable for processing sequences like sentences. The image captioning system was implemented in a similar manner but using a \textbf{convolutional neural network} (CNN) as encoder.


\subsection{Convolutional neural networks}

Since the publication of the AlexNet in 2012 \cite{AlexNet}, CNNs have gained large attention in computer vision for achieving state-of-the-art in various tasks such as object-detection, segmentation, video classification and object tracking \cite{InceptionV3}.

\input{figures/filter.tex}

% TODO add citations

In CNNs, each layer has a width, height and depth. Like in MLPs, the input layer has the width and height of the input image while the depth correspond to the number of color channels (1 for gray scale, 3 for RGB and 4 for RGBA). We now refer to each number in the network as a \textbf{unit} instead of a neuron.

As indicated by the name, CNNs mainly consist of \textbf{convolutional layers}. The depth of a convolutional layer is determined by how many filters it has. Each filter produces a 2-dimensional \textbf{activation map}, see Figure \ref{fig:filter}. Each unit in the activation map comes from a weighted sum over a 3-dimensional region in the previous layer over its full depth. For example a 3x3 filter applied to a 32 depth layer has $3 \cdot 3 \cdot 32=288$ weights and one bias term.
Like with MLP, the sum is passed to to a non-linear activation function such as $\tanh$. The 3x3 regions for each unit are overlapping.
The activation maps for all filters are stacked together to form a 3-dimensional output, which is passed to the next layer.

Because a filter uses the same weights for all sub-regions in the previous layer, the total number of parameters are dramatically reduced compared with a fully connected layer as in an MLP. Moreover, because the same filter is applied to each sub-region, it does not matter where in the input image the object is located.

Units in higher layers in the CNN receive information from a larger \textbf{receptive field} in the input image. Therefore, the filters in lower layers typically learn to detect low-level features like edges and curves while filters in higher layers can detect more complex objects like cats or faces.
By synthesizing the input image to maximize a specific filter's activation, it is possible to get an idea of what the filter has learned to recognize \cite{VisualizeCnn}.

% Write how pooling and convolutions affect receptive field.

\input{figures/maxpool.tex}

Besides filters, CNNs also use \textbf{pooling layers} in order to increase the receptive field without increasing the number of parameters in the model.
Separately for each activation map, pooling aggregates a 2 dimensional region of units (e.g. 2x2) into a single unit, typically by taking the maximum value; see Figure \ref{fig:maxpool}. These regions are non-overlapping so a 2x2 pooling layer decreases the representational size by 4. Thus another benefit of pooling layers is reduced computation time.

At the end of the network, there are often a few fully connected layers (MLP) where the last layer represent the output of the network, for example pseudo-probabilities for different categories using softmax like discussed previously.

\subsubsection{Network architecture for faster computation}

The performance of CNNs typically increases with greater width and depth but it comes at a higher computational cost \cite{InceptionV3}. However, by choosing a good network architecture the same performance can be achieved but at a substantially lower computational cost, for example by refactoring large filters (5x5, 7x7) into consecutive small filters (3x3) and extensive use of pooling layers to reduce dimensionality. The authors also suggest a balance between depth and width of the network.

\subsubsection{Zero padding}

Applying a 3x3 filter to an input image of size 10x10 will output an image of size 8x8 since there are only 8 positions in a single row where the filter can fit. For deep networks, this shrinking might cause the final representation to become too narrow.
One can pad each activation map with zeroes to instead maintain the same size.

\subsection{Attention models}
\label{ssec:attention}
% Since waypointing only needs high-level information in a limited domain, complete transcription is unnecessary. Instead we can use recent advances in image classification.

% TODO write this part again in more detail, especially after spending more time with the literature here.

In contrast to CNNs, the human eye doesn't process the entire scene with equal precision but focuses on the most relevant parts \cite{DeepMindAttention}.
Similarly, we can let the system pay \textbf{attention} to a small part of the image at a time and successively increase the system's understanding of the image.
By choosing good locations for our attention, the important parts of the image are captured while ignoring irrelevant parts.
This reduces the computation time for training and also increases the precision of the network.
% The process of how to select a location to focus on is called an \textbf{attention model} and is often implemented using a \textbf{recurrent neural network} (RNN).
Besides image classification, attention models have also been highly successful for image captioning \cite{AttendAndTell} and machine translation \cite{machine_translation_attention}.

\subsubsection{Details}
We now describe how the attention model in \cite{AttendAndTell} works in more detail.
The goal of the attention model is to produce a vector $\mathbf{z} \in \mathbb{R}^D$ that represents the entire input image by weighing different parts of the output from the encoder CNN.

The encoder output is 3-dimensional but can be seen as a grid of $L$ feature vectors. Each feature vector $\mathbf{a_i} \in \mathbb{R}^D$ corresponds to a specific location in the input image. We feed each $\mathbf{a_i}$ to an MLP which produces a scalar $e_i \in \mathbb{R}$. In the paper, the decoder uses an RNN whose hidden state is also used as input to the MLP besides the feature vector $\mathbf{a_i}$.
The scalar attention weight $\alpha_i$ for each location can then be computed by taking the softmax of $e_i$ over all units:

\[
\alpha_i = \frac{ \exp(e_i) }{ \sum_{k=1}^L \exp(e_i) }
\]

\paragraph{Soft attention}
By definition, the attention weights sum to one. A \textbf{soft attention} model computes the representative vector $\mathbf{z}$ by summing over the feature vectors $\mathbf{a_i}$, using attention as weights:

\[
\mathbf{z} = \sum_{k=1}^L \alpha_i \mathbf{a_i}
\]

\paragraph{Hard attention}
In contrast, a \textbf{hard attention} model interprets the ${\alpha_i}$ as a probability distribution over locations $i$. By sampling this distribution, $\mathbf{z}$ is chosen to be the sampled unit $\mathbf{a_i}$, ignoring the rest of the image.


\subsection{CNNs in multi-digit recognition}

Digit recognition has been studied extensively, especially for recognition of zip-codes in the US postal service \cite{lecun_1989, lecun_1990}. Originally, the digits were segmented manually and linearly transformed to a fix input size of 16x16 pixels.
Each digit image was then classified using a CNN with 3 or 4 hidden layers, with 2x2 pooling between each convolutional layer.

A complete zip-code recognition system locates the place of the zip-code and segments the zip-code into digit images \cite{zipcode_system}. The segmentation is performed by finding \textbf{connected components} (CC) in the image. If the CC has a high confidence in classification it is removed, otherwise it is either split or combined with adjacent CCs until a segmentation has been found where each classification has a high enough confidence. This is done by building a directed acyclic graph where each proposed segment is a node. The length of a path is defined as the product of the classification confidence for each node in the path. The best segmentation is then the path of greatest length in the graph. In order to avoid redundant computations, the segmentation can be done indirectly after the convolutional layers instead of on the input image \cite{lecun_multidigit}.

More recently, deep neural networks have achieved state-of-the-art in multi-digit recognition in street view images \cite{multidigit_streetview}. Instead of handling localization, segmentation and classification as separate tasks, they solve all of them using a single CNN with 11 layers. The output is modeled as a sequence $S$ of digits with a maximum length $N$. The sequence $S$ consists of a random variable $L=n$ for the length of the sequence and $n$ variables $S_i$, one for each digit. In order to handle images without numbers $L$ is allowed to have the value zero and in order to handle longer sequences than $N$, $L$ has a special value "greater than $N$". For an input image $X$, the system can be trained by maximizing $\log P(S \vert X)$. Classification is similarly done by $\text{argmax}_S P(S \vert X)$.
Because each variable has a very small domain, they can be implemented with independent softmax outputs.
Although this model works well for digit recognition of short sequences $N \leq 5$ as well as OCR in CAPTCHAs, the authors speculate that the method is not suitable for long or unbounded sequences.

Another suggested approach to digit recognition in street view is to combine CNNs with HMMs \cite{multidigit_streetview_CNN_HMM}. They use a sliding window to extract multiple overlapping frames on the input image, classify each frame using a CNN to either a digit or null and then feed the sequence of frame labels to an HMM.



\subsection{Additional topics}

Here we list some additional recent progress in the field of neural networks in computer vision which is not used in the thesis.

\subsubsection{Batch normalization}

An alternative to Dropout was suggested by \cite{BatchNormalization}. Besides reducing the risk for overfitting, the authors also suggest that it allows for a higher learning rate in deep networks and thus faster convergence.

For some network layer $\mathbf{x} = {x_1, \ldots, x_d}$, we normalize each $x_i$ to zero mean and unit variance over the current mini-batch:

\[
\hat{x_i} = \frac{x_i - E[x_i]}{ \sqrt{\text{Var} [x_i]} }
\]

We then use two learned parameters $\gamma$ and $\beta$ to instead output $z_i$:

\[
z_i = \gamma \hat{x_i} + \beta
\]

The batch normalization can thus be summarized as $\mathbf{z} = f_{BN}(\mathbf{x}; \gamma, \beta)$.

The batch normalization can either be applied before or after the activation function. The authors suggest that the best result is achieved by applying the batch normalization before the non-liner activation $g$ but after the weights $w_{ij}$ and bias $b_i$:

\[
v_i = g\left( f_{BN}\left( b_i + \sum_j w_{ij} u_j; \gamma, \beta \right) \right)
\]

\subsubsection{Visual-semantic alignment}

% Another way to model regions in the input is by using a R-CNN which have also been very successful at image captioning

Another successful approach to image captioning uses a \textbf{region convolutional neural network} (R-CNN) as encoder \cite{VisualSemanticAlignment}.
The original R-CNN method works by proposing 2000 regions in the image that are the most likely to contain an object \cite{RCNN}. Each region is then fed to a regular CNN for encoding before classification.

Since many regions overlap, it is unnecessary to compute the convolutions for them independently \cite{FastRCNN}. Instead the entire input image is processed through the CNN once and the feature vector of each region is extracted from the resulting activation map. After this improvement, the bottleneck is selecting the regions \cite{FasterRCNN}. By making a neural network to select regions from the activation map instead of from the image, the computation speed can be significantly reduced. However, this requires the training data to contain bounding boxes for the objects to detect.

In the case of image captioning, the correspondences between words and regions in the image is unknown so this relationship is modeled by latent variables \cite{VisualSemanticAlignment}.

\subsubsection{Spatial pyramid pooling}

Convolutional layers and pooling layers can handle input images of changing sizes since the filters just slide over the entire image, so the size of the output matches the input.
However, because the last layers are fully connected, they can only handle input of a fix size. A common solution to this is to rescale or crop the input \cite{FornesCnnCategorization}. If the input consists of segmented images of handwritten words then cropping will cut away critical information while rescaling risk distorting the handwriting beyond recognition. A proposed solution to this is using a layer of \textbf{spatial pyramid pooling} (SPP).
% TODO describe SPP in more detail if relevant

\subsubsection{Spatial transformer networks}

% TODO describe Spatial transformer networks and what they are good for.

\subsubsection{Residual networks}

% TODO cite Microsoft ResNet for architecture of Residual modules. There are also residual networks of residual networks.
