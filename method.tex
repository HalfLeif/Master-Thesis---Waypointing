\section{Method}

\subsection{Network architecture}

\input{figures/system_overview.tex}

We now describe the different parts of the proposed network. See figure \ref{fig:sys_overview} for an overview of how the different parts of the network interact.

\subsubsection{Encoder}

\input{figures/encoder.tex}

For encoder, we use a CNN with 7 convolutional layers with additional pooling layers, see figure \ref{fig:encoder}.
% We experimented with several different variations in number of convolution layers, pooling layers and layer depths before choosing this model.
Two consecutive 3x3 convolutional layers is a refactoring of a single 5x5 layer as suggested by \cite{InceptionV3}. Thus, the first four convolutional layers correspond to two 5x5 convolutional layers with pooling, similar to the original single digit CNN classifier \cite{lecun_1989} but with greater depth.

The number of pooling layers, number of convolutional layers and depth per layer were determined experimentally. The final encoder design, as in figure \ref{fig:encoder}, is somewhat similar to the encoder used in \cite{FornesCnnCategorization} except that we use more aggressive pooling. This is necessary because in our work, we input images of entire pages while \cite{FornesCnnCategorization} classify word images which presumably are much smaller.

%The first four layers correspond closely to the original single digit classifier CNN \cite{lecun_1989} with the exception

% The number of layers and their depth was determined experimentally on the synthesized dataset using previous work as starting point \cite{FornesCnnCategorization}.



\subsubsection{Attention model}

Another major difference between our work and \cite{FornesCnnCategorization} is that although we both use variable input size, they use \textbf{Spatial pyramid pooling} while we apply a soft attention model as presented in \cite{AttendAndTell}. Like we discussed previously in section \ref{ssec:attention}, the encoder output can be seen as a list of feature vectors $\mathbf{a_i} \in \mathbb{R}^D$ which correspond to different but overlapping locations. The number of produced feature vectors depends on the size of the input image. We will see that this attention model aggregates all feature vectors $\mathbf{a_i}$ into a single fixed size vector $\mathbf{z} \in \mathbb{R}^D$.

The attention model consists of a multilayer perceptron which for each feature vector $\mathbf{a_i}$ computes a salience score $e_i$:

\[
e_i = f_\text{MLP}(\mathbf{a_i})
\]

Note that this MLP does not have access to any information about the corresponding location of $\mathbf{a_i}$, thus the network must learn to recognize important parts by looking at the encoded features alone instead of learning to always look at a certain position in the input image. This is especially important for population records where the relevant information can be in different locations depending on scribe and time period.

As discussed, we normalize the $e_i$ into $\alpha_i$ by computing the softmax. Then we use the attentions $\{\alpha_i\}$ as weights over the feature vectors:

\[
\mathbf{z} = \sum_{k=1}^L \alpha_i \mathbf{a_i}
\]

Note that the spatial information about where in the image $\mathbf{z}$ comes from is lost in this sum. So just like the attention MLP, the decoder also becomes invariant to the object's position in the input image.

\subsubsection{Decoder}

The decoder employs an MLP for classifying the aggregated feature vector $\mathbf{z}$ from the attention model.
If $\mathbf{z}$ had a variable length, then we would need a different number of weights in the MLP for each different input size. Since the attention model produces a fixed size output independent of input size we avoid the problem of variable number of parameters.

The proposed MLP consist of one fully connected layer of $1024$ neurons with dropout as well as three parallel readout layers $\mathbf{d}_1$, $\mathbf{d}_2$ and $\mathbf{d}_3$. Each of these three readout layers has a size of $10$ neurons and thus correspond to a single digit.
Because the years we will extract are all in the range 1627--1890, $3$ digits is more than enough to classify year.

We do softmax on each readout layer to get pseudo-probabilities for the digits.
To compute the probability for a specific 3-digit sequence $[x, y, z]$ we simply multiply the probabilities of each digit:

\[
P(d_1=x, d_2=y, d_3=z) = P(d_1=x) P(d_2=y) P(d_3=z)
\]


\subsection{Training procedure}
% TODO Perhaps make this a different section?

\subsubsection{Multi-year labels}
% TODO Should this be part of the description of the dataset or not?

In order to make things smoother in Tensorflow, the labels for each image should have the same size. One way to achieve this is to simply select one of the multiple years and ignore the presence of other years. However, a single year does not accurately represent the image if it contains multiple years which is often the case.

Instead we label each image with a tuple of the smallest and the largest year present in the image. This representation has the benefit of having a fix size independently of how many years are in the image. Furthermore, the entire sequence of years is represented. The only information that is lost is the occasional gap in year sequences. Gaps in year sequences in the Swedish dataset are often very small and they are also quite rare. Thus we assume ignoring gaps has a negligible effect.

% TODO find gap example, how does that image look like? Does it contain the year anyway although it is not written in the label? Exactly how rare is it?

%Although it is not optimal, we currently train the network using one year as label for each image.


\subsubsection{Loss function}

% cross entropy
% cluster error didn't work, because digits are actually independent. Will always have clusters.

\subsubsection{Evaluation}

We evaluate the models on the test set and evaluation set of the Swedish records. Although the models only output a single year per image, we count it as $100\%$ correct if and only if the year with maximum probability is inside the label interval. For example if the label is the interval $(1771, 1774)$ and the network outputs $1773$ we count it as correct. With this definition of correctness we calculate the accuracy over the images in the respective dataset.


\subsection{Tensorflow}
% TODO should technologies/implementation be described in a different section?
% Is it at all relevant to discuss about Tensorflow?

% TODO Mention GPU acceleration here or in section about neural networks or Hardware?

\subsection{Hardware}
% TODO Mention hardware and training time .
