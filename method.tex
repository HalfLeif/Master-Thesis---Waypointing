\chapter{Method}

\textit{TODO...}

\section{Network architecture}

\input{figures/system_overview.tex}

We now describe the different parts of the proposed network. See figure \ref{fig:sys_overview} for an overview of how the different parts of the network interact.
The network architecture is mainly inspired by the work of \textcite{multidigit_streetview} who successfully detect house numbers in Google Street View and \textcite{AttendAndTell} who introduced the attention mechanism that we use.

\subsection{Encoder}

\input{figures/encoder.tex}

For encoder, we use a CNN with 7 convolutional layers with additional pooling layers, see figure \ref{fig:encoder}.
% We experimented with several different variations in number of convolution layers, pooling layers and layer depths before choosing this model.
The original single-digit CNN classifier used two $5 \times 5$ convolutional layers with pooling after each layer \cite{lecun_1989}.
We use a similar beginning of the encoder except that each $5 \times 5$ layer has been refactored into two $3 \times 3$ layers as suggested by \textcite{InceptionV3}. Therefore, the first layers of the encoder should be able to recognize individual digits; we intend the remaining layers to encode a short sequence of digits.

% Two consecutive $3 \times 3$ convolutional layers is a refactoring of a single $5 \times 5$ layer as suggested by \textcite{InceptionV3}. Thus, the first four convolutional layers correspond to two $5 \times 5$ convolutional layers with pooling, similar to the original single-digit CNN classifier \cite{lecun_1989} but with greater depth.

After each pooling layer, we increase the number of filters in order to maintain a good size for the representation of the input. Per recommendation of \textcite{InceptionV3}, we let the representational size be somewhat larger in the beginning of the network and then slowly decrease. The representation should decrease as the information in the previous layer is abstracted.

%There is a balance between the number of layers in the encoder and the width of each layer.
In the experience of \textcite{InceptionV3}, a network is the most computationally efficient when the depth and width of a network is kept in balance. Consequently, we experiment with different number of layers and filters for each layer in section \ref{sssec:exp_encoder}.

% As \textcite{InceptionV3} also recommended, we try to keep a healthy balance between the depth and the width of the network.
%They suggest that a network is most computationally efficient when
% the number of filters in each layer.
% A deep network has many layers
% A shallow network where each layer is very thick can be refactored into a more efficient network with more layers. However, if the network is very deep but thin, the representation at each layer may be insufficient to encode the necessary information.

% The number of pooling layers, number of convolutional layers and depth per layer were determined experimentally.

% Optimally, we want all 4 digits of a year to fit in a single receptive field.
% To achieve this, we use many $2 \times 2$ pooling layers

The final encoder design, depicted in figure \ref{fig:encoder}, is somewhat similar to the encoder used by \textcite{FornesCnnCategorization} except that we use more aggressive pooling. The additional pooling layers are necessary because we input images of entire pages, while \textcite{FornesCnnCategorization} classify word images, which presumably are much smaller.
Another difference is that we use a $1 \times 5$ convolutional layer at the end of the encoder in order to increase the width of the receptive field. We want the receptive field to be wide in order to capture as mush as possible of each  digit sequence in a single output unit.
% , which increases the width of the receptive field without increasing its height.

%The first four layers correspond closely to the original single digit classifier CNN \cite{lecun_1989} with the exception

% The number of layers and their depth was determined experimentally on the synthesized dataset using previous work as starting point \cite{FornesCnnCategorization}.



\subsection{Attention model}

Another major difference between our work and \cite{FornesCnnCategorization} is that although we both use variable input size, they use \textbf{spatial pyramid pooling}, while we apply a soft attention model as presented in \cite{AttendAndTell}.
Like previously mentioned in section \ref{ssec:attention}, the encoder output can be seen as a list of feature vectors $\mathbf{a_i} \in \mathbb{R}^D$, which correspond to different, but overlapping, locations in the input image. The number of produced feature vectors depends on the size of the input image. We will see that this attention model aggregates all feature vectors $\mathbf{a_i}$ into a single fixed size vector $\mathbf{z} \in \mathbb{R}^D$.

The attention model consists of a multilayer perceptron, which for each feature vector $\mathbf{a_i}$ computes a salience score $e_i$:

\[
e_i = f_\text{att}(\mathbf{a_i})
\]

Note that this MLP does not have access to any information about the corresponding location of $\mathbf{a_i}$, thus the network must learn to recognize important parts by looking at the encoded features alone instead of learning to always look at a certain position in the input image. This is especially important for population records, where the relevant information can be in different locations depending on scribe and time period.

% We normalize the $e_i$ into $\alpha_i$ by computing the softmax.
% We normalize the $e_i$ by softmax and call the corresponding values attentions

We denote the softmax of $e_i$ as attentions $\alpha_i$, which sums to one by definition of softmax.
%We compute the softmax of $e_i$ as $\alpha_i$, which sums to one per definition.
We then use the attentions $\alpha_i$ as weights over the feature vectors:

\[
\mathbf{z} = \sum_{k=1}^L \alpha_i \mathbf{a_i}
\]

% Note that the spatial information about where in the image $\mathbf{z}$ comes from is lost in this sum.

%So just like the attention MLP, the decoder also becomes invariant to the object's position in the input image.

\subsection{Decoder}

The decoder employs an MLP for classifying the aggregated feature vector $\mathbf{z} \in \mathbb{R}^D$ from the attention model.
If $\mathbf{z}$ had a variable length, then we would need a different number of weights in the MLP for each different input size. Since the attention model produces a fixed size output independent of input size we avoid the problem of variable number of parameters.

Because $\mathbf{z}$ is summed over all locations, it does not contain any spatial information about where detected features come from.
Thus, the decoder is invariant to the location of detected objects, just like the attention model.

The proposed decoder MLP consists of an input layer of size $D$, one fully connected layer of $1024$ neurons with dropout and three parallel readout layers $D_1$, $D_2$ and $D_3$. Each of these three readout layers has a size of $10$ neurons and corresponds to a single digit in the range zero to nine.
Because the years we will extract are all in the range 1627--1890, $3$ digits is more than enough to classify year.

We do softmax on each readout layer to get pseudo-probabilities for the digits.
To compute the probability for a specific 3-digit sequence $\langle x, y, z \rangle$ we simply multiply the probability of each digit:

\[
P(D_1=x, D_2=y, D_3=z) = P(D_1=x) P(D_2=y) P(D_3=z)
\]


\section{Training procedure}

Here we describe how we train and evaluate the different models.

\subsection{Multi-year labels}

As presented in section \ref{sssec:swe_multiyear}, many images contain records from multiple years. However, in order to simplify the implementation, we would like all image labels to have the same size.
One way to achieve this is to simply select one of the multiple years in an image and ignore the presence of other years. However, a single year does not accurately represent images that contain multiple years.

Instead, we label each image with a tuple of the smallest and the largest year present in the image. This representation has the benefit of having a fixed size independently of how many years are in the image. Furthermore, the entire sequence of years is represented. The only information that is lost is the occasional gap in year sequences. Such gaps are often very small and also quite rare in the Swedish dataset so we assume ignoring them has a negligible effect on training and evaluation.

% TODO find gap example, how does that image look like? Does it contain the year anyway although it is not written in the label? Exactly how rare is it?

%Although it is not optimal, we currently train the network using one year as label for each image.


\subsection{Loss function}

As mentioned above, we encode the label as a tuple of min and max year. Although it is not optimal to ignore the other years, our base loss function  only uses the maximum year.
The reason for taking the maximum year instead of the minimum is that during the indexing process, the indexer could have inferred the minimum year from a previous page. In such a case, the minimum year would not be written anywhere on the current page. However, if there is more than one year for an image, all years except the first must be present on the page to show where the new year starts. See figure \ref{fig:page} for an example.

We observed previously in section \ref{sssec:CrossEntropy} that cross-entropy often outperforms squared error as a loss function for classification. Cross-entropy compares a single label against the probability distribution from the network's readout layer. In our case, we output three probability distributions, one for each digit.
% For a single label per image, we then have two options.
Taking the maximum year as our label, we compute
%The first option is to take
the cross-entropy for each digit and sum the losses.

In section \ref{ssec:alt_loss} we present and discuss some alternatives for loss function on this task. Some of the alternative loss functions we discard by theory and the others we evaluate in section \ref{sec:experiments}.

\subsection{Evaluation}

We evaluate the models on the test set and evaluation set of the Swedish records. Although the models only output a single year per image, we count it as $100\%$ correct if and only if the year with maximum probability is inside the label interval. For example if the label is the inclusive interval $[1771, 1774]$ and the network outputs $1773$, we count it as correct. With this relaxed definition of correctness we calculate the accuracy over the images in the respective dataset.

Another possible metric is measuring the numeric distance between the prediction and the label; in the case of multiple years we choose the closest one. Although we will see that numeric distance is not very useful as a loss function, it can give an indication whether the model can predict the correct decade.

% TODO precision/recall if uses confidence boundary

\input{method_extensions.tex}
