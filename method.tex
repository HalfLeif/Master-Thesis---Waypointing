\section{Method}

\subsection{Network architecture}

\input{figures/system_overview.tex}

We now describe the different parts of the proposed network. See figure \ref{fig:sys_overview} for an overview of how the different parts of the network interact.

\subsubsection{Encoder}

\input{figures/encoder.tex}

For encoder, we use a CNN with 7 convolutional layers with additional pooling layers, see figure \ref{fig:encoder}.
% We experimented with several different variations in number of convolution layers, pooling layers and layer depths before choosing this model.
Two consecutive 3x3 convolutional layers is a refactoring of a single 5x5 layer as suggested by \cite{InceptionV3}. Thus, the first four convolutional layers correspond to two 5x5 convolutional layers with pooling, similar to the original single digit CNN classifier \cite{lecun_1989} but with greater depth.

The number of pooling layers, number of convolutional layers and depth per layer were determined experimentally. The final encoder design, as in figure \ref{fig:encoder}, is somewhat similar to the encoder used in \cite{FornesCnnCategorization} except that we use more aggressive pooling. This is necessary because in our work, we input images of entire pages while \cite{FornesCnnCategorization} classify word images which presumably are much smaller.

%The first four layers correspond closely to the original single digit classifier CNN \cite{lecun_1989} with the exception

% The number of layers and their depth was determined experimentally on the synthesized dataset using previous work as starting point \cite{FornesCnnCategorization}.



\subsubsection{Attention model}

Another major difference between our work and \cite{FornesCnnCategorization} is that although we both use variable input size, they use \textbf{Spatial pyramid pooling} while we apply a soft attention model as presented in \cite{AttendAndTell}. Like we discussed previously in section \ref{ssec:attention}, the encoder output can be seen as a list of feature vectors $\mathbf{a_i} \in \mathbb{R}^D$ which correspond to different but overlapping locations. The number of produced feature vectors depends on the size of the input image. We will see that this attention model aggregates all feature vectors $\mathbf{a_i}$ into a single fixed size vector $\mathbf{z} \in \mathbb{R}^D$.

The attention model consists of a multilayer perceptron which for each feature vector $\mathbf{a_i}$ computes a salience score $e_i$:

\[
e_i = f_\text{MLP}(\mathbf{a_i})
\]

Note that this MLP does not have access to any information about the corresponding location of $\mathbf{a_i}$, thus the network must learn to recognize important parts by looking at the encoded features alone instead of learning to always look at a certain position in the input image. This is especially important for population records where the relevant information can be in different locations depending on scribe and time period.

As discussed, we normalize the $e_i$ into $\alpha_i$ by computing the softmax. Then we use the attentions $\{\alpha_i\}$ as weights over the feature vectors:

\[
\mathbf{z} = \sum_{k=1}^L \alpha_i \mathbf{a_i}
\]

Note that the spatial information about where in the image $\mathbf{z}$ comes from is lost in this sum. So just like the attention MLP, the decoder also becomes invariant to the object's position in the input image.

\subsubsection{Decoder}

The decoder employs an MLP for classifying the aggregated feature vector $\mathbf{z}$ from the attention model.
If $\mathbf{z}$ had a variable length, then we would need a different number of weights in the MLP for each different input size. Since the attention model produces a fixed size output independent of input size we avoid the problem of variable number of parameters.

The proposed MLP consist of one fully connected layer of $1024$ neurons with dropout as well as three parallel readout layers $\mathbf{d}_1$, $\mathbf{d}_2$ and $\mathbf{d}_3$. Each of these three readout layers has a size of $10$ neurons and thus correspond to a single digit.
Because the years we will extract are all in the range 1627--1890, $3$ digits is more than enough to classify year.

We do softmax on each readout layer to get pseudo-probabilities for the digits.
To compute the probability for a specific 3-digit sequence $[x, y, z]$ we simply multiply the probabilities of each digit:

\[
P(d_1=x, d_2=y, d_3=z) = P(d_1=x) P(d_2=y) P(d_3=z)
\]


\subsection{Training procedure}
% TODO Perhaps make this a different section?

Here we describe how we train and evaluate the different models.

\subsubsection{Multi-year labels}
% TODO Should this be part of the description of the dataset or not?

In order to make things smoother in Tensorflow, the labels for each image should have the same size. One way to achieve this is to simply select one of the multiple years and ignore the presence of other years. However, a single year does not accurately represent the image if it contains multiple years which is often the case.

Instead we label each image with a tuple of the smallest and the largest year present in the image. This representation has the benefit of having a fix size independently of how many years are in the image. Furthermore, the entire sequence of years is represented. The only information that is lost is the occasional gap in year sequences. Gaps in year sequences in the Swedish dataset are often very small and they are also quite rare. Thus we assume ignoring gaps has a negligible effect.

% TODO find gap example, how does that image look like? Does it contain the year anyway although it is not written in the label? Exactly how rare is it?

%Although it is not optimal, we currently train the network using one year as label for each image.


\subsubsection{Loss function}

We observed previously in section \ref{sssec:CrossEntropy} that cross-entropy often outperforms squared error as a loss function for classification. Cross-entropy compares a single label against the probability distribution from the network's readout layer. In our case, we output three probability distributions, one for each digit.
For a single label per image, we then have 2 options.
The first option is to take the cross-entropy for each digit and then sum the losses.
The other option is to take the Cartesian product of the three distributions into a single large distribution and take the cross-entropy of the combined distribution. A comparison between these two approaches is performed in section \ref{sssec:ind_digits}.

\subsubsection{Evaluation}

We evaluate the models on the test set and evaluation set of the Swedish records. Although the models only output a single year per image, we count it as $100\%$ correct if and only if the year with maximum probability is inside the label interval. For example if the label is the inclusive interval $[1771, 1774]$ and the network outputs $1773$, we count it as correct. With this relaxed definition of correctness we calculate the accuracy over the images in the respective dataset.

% TODO median distance

% TODO precision/recall if uses confidence boundary?
% problematic for multi-years: the confidence is split between different outputs...

\subsection{Additional ideas}

\subsubsection{Multi-year loss}

It is not trivial to make a good loss-function for multi-year labels. If we sum the loss for each year in the label, the images with many years have a higher influence than images with few years. Instead we can propose to take the average loss over the years but then another problem emerges: if for example the network would correctly classify an image as $1772$ where the label is $[1771, 1774]$, we still punish the network for not finding the other years.

% TODO write about experiment regarding multiyear loss.

\subsubsection{Distance loss}

Sometimes the network makes predictions that are very far away from the correct year, for example $1774$ instead of $1666$. Although we want to get the exact year, $1663$ could be considered a much better suggestion than $1774$.

One suggested loss for distance would be squared distance. For a single year label $y$, and a probability distribution $P(X=x)$ over the years we add a distance loss $L_D$:

\[
L_D(X, y) = \sum_{x \in [1000, 1999]} (y-x)^2 P(X=x)
\]

This definition of a distance loss has the wanted property that predictions far away have a big loss while predictions close to the label only give a small loss. However, there are two major problems: 1. bias and 2. independent digits.

For $L_D$ to be unbiased, the loss over a uniform distribution $U$ should be the same for all labels $y$ in the allowed interval. However, this is not the case. We can calculate the bias for different $y$ by summing $L_D$ over the allowed range. For simplicity, we map the integral year range $[1000, 1999]$ to a continuous range $[0,1]$ and integrate:

\[
L_D(U, y) = \int_0^1 (x-y)^2 P(U=x) dx = \frac{1}{3} + y(1-y)
\]

Thus we see that for the above definition of $L_D$, there is a strong bias for labels close to the center of the range. Likewise, a network trained with this distance loss becomes biased to suggesting years closer to the middle of the output range.

The second problem with this distance loss is that even for a perfect network, there will be considerable predictions far away from the true label because we multiply the probability distributions for the digits independently. That is, if the network is $100\%$ certain that the last two digits should be $3, 4$ and the correct label is $1834$, we will still get strong signals for $1734$ and $1634$.

\subsubsection{Distance score}

In order to solve the first problem we could suggest another approach: instead of minimizing a distance error, we can maximize a proximity score $S_D$ using a weight function $W$ around the label $y$:

\[
S_D(X, y) = \sum_i P(X=x) W(x, y)
\]

Because we want to reward predictions near $y$, we can use a truncated Gaussian as weight function:

\[
W(x, y) = \exp \left( \frac{-(y-x)^2}{\sigma^2} \right)
\]

We can then compute the bias in the same way as before, by integrating over a mapped interval $[0,1]$:

\[
S_D(U, y) = \int_0^1 P(U=x) W(x, y) dx =
\int_{-y}^{1-y} \exp \left( \frac{-x^2}{\sigma^2} \right) dx
%\frac{\sigma \sqrt{\pi}}{2} \left(
%\text{erf} \left( \frac{1-y}{\sigma} \right) +
%\text{erf} \left( \frac{y}{\sigma} \right)
%\right)
\]

We see that there is a bias for labels $y$ near the edges of the range but for most values of $y$, the bias is negligible for small values of $\sigma$.


\subsection{Tensorflow}
% TODO should technologies/implementation be described in a different section?
% Is it at all relevant to discuss about Tensorflow?

% TODO Mention GPU acceleration here or in section about neural networks or Hardware?

\subsection{Hardware}
% TODO Mention hardware and training time .
