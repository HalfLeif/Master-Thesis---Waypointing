\section{Datasets}

Here we describe some existing datasets for historical handwriting recognition and information extraction. We also consider their usefulness in regard to the thesis.

% TODO add pictures for some of the datasets?


\subsection{MNIST}

The MNIST database contains $70000$ normalized images of handwritten digits \cite{MNIST_orig}. All images have the fix size $28 \times 28$ pixels. The dataset is very popular for testing new algorithms and machine learning models \cite{MNIST}.
% It is even used for the introductory tutorial of Tensorflow \cite{}.
% https://www.tensorflow.org/get_started/mnist/pros
Although we can not use MNIST to evaluate the proposed models in this thesis, the dataset is useful for quick experimentation and pre-training. We discuss this further in section \ref{ssec:pretrain}.

\subsection{Historical documents}
Several collections of historical documents have been line segmented and transcribed for public use \cite{esposalles}: George Washington, Parzival, Saint Gall, RODRIGO and GERMANA. However, they consist of prose written by one or two scribes and not civil population records which are typically written by many different scribes.
% TODO add citations for each mentioned data set?

\subsection{Esposalles}
In contrast, the Esposalles dataset consists of book indices and historical marriage licenses from the Cathedral of Barcelona \cite{esposalles}. Experts have manually segmented and transcribed each word exactly as they occur in the image.

Although the Cathedral of Barcelona holds 291 books containing approximately 600 000 marriage licenses between 1451 and 1905, only a tiny fraction of this collection has been transcribed for the Esposalles dataset. The dataset consists of 173 pages from a single book written by a single scribe, containing 1747 marriage licenses between 1617 and 1619. Additionally, the dataset contains 29 pages of book indices.

The Esposalles dataset was used as ground truth for named entity recognition in the Robust reading competition at the International Conference on Document Analysis and Recognition (ICDAR) 2017 \cite{EsposallesCompetition}. For the competition, the words have been annotated with semantic categories such as surname of husband and surname of wife.

We think that the exact transcriptions, bounding boxes and annotations make the dataset useful for segmentation-based methods in tasks like word segmentation, handwriting recognition and named entity recognition.
Specifically for this thesis, we intend to classify years in a much larger range than 1617 to 1619 so the Esposalles dataset is simply too small. Furthermore, we are interested in exploring segmentation-free methods so we would not take advantage of the bounding boxes present in this dataset.

% TODO perhaps argue about slow and expensive process?
% We argue that creating a dataset of exact transcriptions and annotations by using paleographical experts is too slow and expensive to make a sufficiently large

%Furthermore, we believe that in order to make a system for fully automated indexing, the dataset needs to cover many more examples. Since using paleographical experts to transcribe and annotate each word is a slow and expensive process we do not expect the dataset to grow quickly.
% Thus, we assume that in order to create a sufficiently large dataset for training
%Thus we argue that for a dataset to grow sufficiently large it should
%1. not require the transcriber to be an expert and 2.
%only contain the most vital coarse information instead of every single word.


\subsection{IRIS}

While the Esposalles dataset focus on individual words transcribed by experts, the IRIS dataset contain record information extracted by volunteers in FamilySearch's indexing program \cite{Iris}.
Thus the extracted information does not contain exact transcriptions of the text but rather the most important semantic contents.
For example, abbreviations may have been expanded, dittos replaced by their intended values and information that was deemed genealogically irrelevant has been ignored.
Furthermore, while in Esposalles each transcribed word has a manually created bounding box, in IRIS there is no indication about what part of the image the information was extracted from.

The IRIS dataset consist of four collections of population records, totaling nearly $50000$ images:

\paragraph{1, 2}
The first two collections in the IRIS dataset are the 1930 population census in the US and Mexico. The images consist of big tables with names and occupations in the cells.
For the purpose of this thesis where we extract the written year for each page, the 1930 censuses are not directly useful since they only contains records from a single year. Furthermore, it is not clear that if a system can learn from this collection that it would by useful for any other collection.
However because the 1930 censuses are written on printed tables, word segmentation should be considerably simpler than for the free-form writing as in other collections.

\paragraph{3}
The third collection contains marriage licenses between 1837-1944 in Arkansas. The marriage licenses are printed templates with handwritten text in the blanks.
It is likely that the system would learn to recognize the printed template and hence it would not be able to generalize to other collections that use different templates or free-form writing. If many collections were using the same template it would be useful to learn but then one might as well hard-code the positions of the blanks in the template. Thus, it is not very useful as training-data for models in this thesis.

\paragraph{4}
The last collection contains more than 10000 pages with records of baptisms, burials and marriages from French parishes between 1533-1906. Some of the pages contain printed templates but the majority seems to be free-form handwriting.
From manually inspecting these images, we conclude that many of them do not contain a written year anywhere on the page. These are problems that a future automated extraction system would need to handle, for this thesis however we settle for another dataset that seems somewhat easier.


\subsection{Swedish population records}

\input{figures/collections}

In cooperation with FamilySearch for making this thesis, we have been granted access to images and the corresponding extracted information from six collections of Swedish population records.
These collections contain records of baptisms, burials and marriages in six regions between 1627 and 1890, see Table \ref{tab:collections}. They are the Swedish counterpart of the French IRIS collection mentioned above.

\input{figures/page.tex}

\subsubsection{The indexing process}

Just like with the IRIS dataset, the indexers have not provided us with bounding boxes or exact transcriptions but with extracted genealogical information. An indexed baptism record typically contains the name of the newborn child, the names of the parents, the date of birth, the date of baptism and location of the parents' home. The ecclesiastical records typically contain the names of witnesses to the baptism but this is ignored in the indexing process.

Each page is indexed by two independent volunteers. Any conflicts between the two versions is resolved by a third volunteer before submission.
The double work and review process increases the quality and consistency of the extracted information.
% Duplicating the indexing work and quality-checking increases the quality of the indexed data.
% Thus, the indexed information maintains a high quality even though it is not performed by professionals.

The extracted information is stored in the compressed GedcomX\footnote{\url{http://www.gedcomx.org/About.html}} format which has been developed specifically for genealogical information.
Each event of baptism, burial or marriage is stored as a record organized in the above mentioned six collections. Within a collection, the records do not seem to be stored in any particular order. Each record however contains information about which image it belongs to as well as some meta-information about the book the record comes from.

\subsubsection{Varying image size}

The images are stored in the JPG-format using lossy compression. Each image has a different size although the majority are about $5700 \times 4500$ pixels. There are also outliers with sizes like $7262 \times 5907$ pixels and $1885 \times 5110$ pixels. We think the varying image sizes is a result of cropping the image to fit the book in the photograph.
Since different books have different proportions, the aspect ratio of the images differs somewhat although it is often close to $1.25$:$1$.

\subsubsection{Page structure}

Each image consist of two pages. The pages are always handwritten. Sometimes the pages have pre-printed headers and vertical lines for columns, other times they are entirely free-form.
The year can often be found near the top of the page either to the left or in the middle, see Figure \ref{fig:page} for an example.

\input{figures/aar.tex}

Although the general structure is the same for all six collections, the exact page layout differs greatly both within and between collections. Another difference relevant for the neural network is that sometimes the year is following the word "\r{A}r" although the handwriting style for the word can be very different depending on time and place, see figure \ref{fig:aar} for some examples.


\subsubsection{Multiple years per page}

 % Additionally, when the year changed the scribe often indicated so by writing the new year in big letters before the block of new records.
Typically, the scribes did not use a new page for every new year but indicated the change of year by writing the new year in big letters before continuing to write the new records; an example of this can be seen in Figure \ref{fig:page}.
Thus besides at the top, written years can be found in virtually any part of the page. Another consequence is that a page can contain several years in a sequence like $1771, 1772, 1773, 1774$. This is especially true for small parishes where there may not have been so many burials per year. Out of the $43068$ indexed images, $16074$ or $37.3\%$ have more than one year in the extracted information.

Some images however do not contain a written year, the year is simply implied from previous pages. For such instances, indexers are instructed to search the previous and following image for an indication of the year. If the implied year is found, it is noted in the extracted information, otherwise it is indicated as unknown.
If the year is unknown or the image is blank, we ignore the image because we do not have a label for it and we expect that the year is anyway not indicated in the image. Of the indexed images, $471$ or $1\%$ have no year label.

However, in the extracted information there is no way to distinguish whether the extracted year came from the same image or if it was implied from a neighboring image. Thus, it is inevitable that some images in the dataset have labels although the year is actually not written in the image. One way to counter this problem would be to only train on images that are labeled with more than one year, expecting an indicated year change somewhere in the image. Doing so would however reduce the dataset to $37.3\%$.
Although we have no way to calculate how often the label implies a year that is missing from the image without manually inspecting every image, we assume that it is relatively rare and thus negligible.
