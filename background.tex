
\section{Related work}

%% Rewrite this part, add more recent work, especially in deep learning and CNNs: spatial transformations, visual-semantic alignment etc.

% Here, we discuss the field of two problems related to waypointing: HTR and image classification.

\subsection{Handwritten text recognition}

HTR has been solved for recognizing postal addresses on letters and reading bank cheques. This has been possible because 1. the text is in a very small domain and 2. these applications have high market value \cite{40_years_HWR}. However, transcribing natural languages in general is still an unsolved problem.

\paragraph{}
HTR solutions can typically be described by these four steps: \cite{offline_HWR_CNN}:
\begin{enumerate}
    \item Pre-processing - each pixel in the image is mapped to either 0 or 1, skew is corrected and noise is removed.
    \item Segmentation - the image is cut into small segments, so that each segment contains a handwritten word.
    \item Feature extraction - each image segment is encoded into some vector of features.
    \item Classification - the feature vector is interpreted as a word from a known vocabulary.
\end{enumerate}

A comprehensible survey of common techniques for the different steps can be found here \cite{HWR_survey}.
Previous work include \textbf{Hidden markov models} (HMMs) in combination with neural networks \cite{Offline_HWR_HMM_ANN}.
More recent approaches include multi-stream HMMs \cite{HWR_multi_stream_HMM_arabic}, HMMs with \textbf{recurrent neural networks} (RNNs) \cite{Offline_HWR_RNN} and deep \textbf{convolutional neural networks} (CNNs) \cite{offline_HWR_CNN}.


% \subsection{Convolutional neural networks}

\subsection{Deep learning}

Both machine translation and image captioning used to be solved by solving each subproblem separately but this kind of approach has been outperformed by end-to-end systems using deep learning \cite{ShowAndTell}. One neural network \textbf{encoded} the input to a fixed-length vector representation which was \textbf{decoded} by another network. In the case of machine translation both the encoder and decoder was implemented by an RNN which are highly suitable for processing sequences like sentences. The image captioning system was implemented in a similar manner but using a \textbf{convolutional neural network} (CNN) as encoder.

\subsection{Convolutional neural networks}

Since the publication of the AlexNet in 2012 \cite{AlexNet}, CNNs have gained large attention in computer vision for achieving state-of-the-art in various tasks such as object-detection, segmentation, video classification and object tracking \cite{InceptionV3}.

In CNNs, each layer of neurons has a width, height and depth. The width and height correspond to that of the input image while the depth depends on the number of \textbf{filters} applied to the previous layer.
% TODO add citation
Each filter performs a weighted sum over a small region in the previous layer, e.g. 3x3 neurons using the full depth. The sum is then passed to a non-linear activation function such as $\tanh$ and serves as input for the next layer.
Because the same weights are reused for all parts of the image, the number of parameters are reduced compared with a fully connected layer as in a standard \textbf{multilayer perceptron} (MLP).

Neurons in higher layers in the CNN receive information from a larger \textbf{receptive field} in the input image. Therefore, the filters in lower layers typically learn to detect low-level features like edges and curves while higher layers can detect more complex objects like cats or faces.
By synthesizing the input image to maximize a specific filter's activation, it is possible to get an idea of what the filter has learned to recognize \cite{VisualizeCnn}.

Besides filters, that is \textbf{convolutional layers}, CNNs also use \textbf{pooling layers} in order to increase the receptive field without increasing the number of parameters in the model.
Pooling simply combines a region of neurons (e.g. 2x2) into a single neuron, typically by taking the maximum value.

At the end of the network, there are often a few fully connected layers (MLP) where the last layer represent the output of the network, for example pseudo-probabilities for different categories.

\subsubsection{Problems}

The performance of CNNs typically increases with greater width and depth but it comes at a higher computational cost \cite{InceptionV3}. However, by choosing a good network architecture the same performance can be achieved but at a substantially lower cost, for example by refactoring large filters (5x5, 7x7) into consecutive small filters (3x3) and extensive use of pooling layers to reduce dimensionality. The authors also suggest a balance between depth and width of the network.

Very deep networks suffer from the \textbf{vanishing gradient} problem. Because $\tanh$ has a very small derivative for large activations, the training can become very slow \cite{AlexNet}. This can largely be solved by using the \textbf{rectified linear} (ReLU) function for activation: $f(x) = \max{0, x}$.

Another problem with large networks is overfitting which means that the network learns the training set by heart instead of learning to recognize the general patterns \cite{AlexNet, FornesCnnCategorization}. The \textbf{dropout} method introduces a probability of setting each neuron's activation to $0$ during training, often with 50\% probability. This forces the network to learn multiple paths for recognizing different features and hence creating a more robust representation. Dropout also reduces the computational cost because fewer neurons participate in each training step.

Applying a 3x3 filter to an input image of size 10x10 will output an image of size 8x8 since there are only 8 positions in a single row where the filter can fit. For deep networks, this shrinking might cause the final representation to become too narrow. To counter this shrinking one can apply \textbf{zero padding}, that is to recreate the outer layer with zeroes after each filter in order to maintain the same size.

Convolutional layers and pooling layers can handle input images of changing sizes since the filters just slide over the entire image, so the size of the output matches the input.
However, because the last layers are fully connected, they can only handle input of a fix size. A common solution to this is to rescale or crop the input \cite{FornesCnnCategorization}. If the input consists of segmented images of handwritten words then cropping will cut away critical information while rescaling risk distorting the handwriting beyond recognition. A proposed solution to this is using a layer of \textbf{spatial pyramid pooling} (SPP).
% TODO describe SPP in more detail in another section if relevant

% TODO describe Spatial transformer networks.

\subsubsection{Attention models}

% Since waypointing only needs high-level information in a limited domain, complete transcription is unnecessary. Instead we can use recent advances in image classification.

% TODO write this part again in more detail, especially after spending more time with the literature here.

In contrast to CNNs, the human eye doesn't process the entire scene with equal precision but focuses on the most relevant parts \cite{DeepMindAttention}.
Similarly, we can let the system pay \textbf{attention} to a small part of the image at a time and successively increase the system's understanding of the image.
By choosing good locations for our attention, the important parts of the image are captured while ignoring irrelevant parts.
This reduces the computation time for training and also increases the precision of the network. The process of how to select a location to focus on is called an \textbf{attention model} and is often implemented using a \textbf{recurrent neural network} (RNN). Besides image classification, attention models have also been highly successful for image captioning \cite{AttendAndTell} and machine translation \cite{machine_translation_attention}.
