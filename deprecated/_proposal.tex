
\section{Introduction}

FamilySearch is a large non-profit genealogy organization \cite{FamilySearchAbout}. They have been photographing handwritten historical population records in many countries for many years. An example of such a photographed record can be seen in Figure \ref{fig:example}.

In 2006, the Indexing project started where volunteers manually extract relevant information from the photographed pages \cite{Indexing}. The extracted information is quality-checked and then published on the organization's website FamilySearch.org. In 2013, over 1 billion data posts for births, deaths and marriages had been extracted (\textbf{indexed}) by volunteers \cite{Billion}.

The number of photographed records grow faster than what the available pool of volunteers can process. Thus, there is a need for automating as much of the indexing work as possible. This is a difficult task because the records come from a vast array of locations and periods in time in different languages, not to mention variations in both spelling and page structure. Although the handwriting style might be similar in the same time period, there is considerable variance between different scribes.

An easier problem to solve than extracting all relevant information in the images is grouping the images by year, record type and location. This task is called \textbf{waypointing} \cite{Waypointing} and it currently requires considerable manual labor by genealogy experts.
Waypointing is an easier problem than handwriting recognition because it only needs to consider high-level information in each image; this information belongs to a small domain compared to the full vocabulary of a natural language.

Besides reducing the human workload, solving waypointing with high precision would constitute a stepping stone towards fully automated indexing which would considerably speed up the genealogic research since the records can be searched for in the database instead of page by page. Thus, the world's genealogic records would be freely and readily accessible to everyone.


\section{Background}

Considerable research has been produced to make computer systems better understand handwritten text and images in general.
%for better understanding handwritten text and images in general.

\subsection{Offline handwriting recognition}

%\textit{TODO: find more recent articles.}

\textbf{Handwriting recognition} (HWR) is the task of transforming some handwritten input to a string of characters, that is a \textbf{transcription} of the text.
%producing a string of characters (\textbf{transcription}) of .
%is the task of correctly \textbf{transcribing} handwritten text, that is transforming some handwriting input into a string of characters.
In online HWR, the pen strokes are captured in real time while in offline HWR only an image of the handwritten text is provided \cite{offline_HWR_CNN}. Because it is difficult to recognize the individual strokes from an image and since the order of strokes is unknown, offline HWR is more difficult than online HWR.
%Because less information is available, offline HWR is more difficult than online HWR.
%Much effort has been put into online HWR \cite{offline_HWR_CNN}

\paragraph{}
Offline HWR typically has four steps \cite{offline_HWR_CNN}:
\begin{enumerate}
    \item Pre-processing - each pixel in the image is mapped to either 0 or 1, skew is corrected and noise is removed.
    \item Segmentation - the image is cut into small segments, so that each segment contains a handwritten word.
    \item Feature extraction - the image is encoded into some vector of features.
    \item Classification - the feature vector is interpreted as a word from a known vocabulary.
\end{enumerate}

A comprehensible survey of common techniques for the different steps can be found here \cite{HWR_survey}.
Previous work include \textbf{Hidden markov models} (HMMs) in combination with neural networks \cite{Offline_HWR_HMM_ANN}.
%\textbf{Hidden markov models} (HMMs) have previously been popular, sometimes in combination with various \textbf{artifical neural networks} \cite{Offline_HWR_RNN, Offline_HWR_HMM_ANN}.
More recent approaches include multi-stream HMMs \cite{HWR_multi_stream_HMM_arabic}, HMMs with \textbf{recurrent neural networks} (RNNs) \cite{Offline_HWR_RNN} and deep \textbf{convolutional neural networks} (CNNs) \cite{offline_HWR_CNN}.

%Offline HWR is the task of transcribing handwritten text in images.

%Current state-of-the-art systems for offline HWR uses Recurrent Neural Networks (RNN) together with hybrid Hidden Markov Models (HMM) \cite{Offline_HWR_RNN}. HMMs have been widely popular for offline HWR for several years after their success on speech recognition \cite{Offline_HWR_HMM_ANN}. The analogy is that the strokes of a pen can be interpreted as a one-dimensional signal, like sound is a one-dimensional signal.

%\newpage

\subsection{Attention models}

% TODO: make technical terms bold, eg. convolutional neural networks

% TODO: introduce Artificial neural networks?

% The human eye focuses on the important part(s) of the field of vision instead of processing the entire scene at once \cite{DeepMindAttention}. 


\textbf{Convolutional neural networks} (CNNs) learn to recognize simple shapes in small overlapping regions of the input image and combines that information in higher layers to successfully detect complex objects without feature engineering \cite{DeepMindAttention}.
However, CNNs require considerable computation power to train.
%can detect objects in a scene without feature engineering because they 
%and the processing time depends directly on the size of the image

In contrast to CNNs, the human eye doesn't process the entire scene with equal precision but focuses on the most relevant parts \cite{DeepMindAttention}.
%The human eye is very good at selecting relevant aspects of images to focus on and ignore the rest. 
%Instead of focusing on the entire image at once, we can let the system choose a small part to focus on at a time and progressively increase its understanding of the image.
Similarly, we can let the system pay \textbf{attention} to a small part of the image at a time and successively increase the system's understanding of the image.
By choosing good locations for our attention, the important parts of the image are captured while ignoring irrelevant parts.
%The selected location can then be processed using a CNN instead of letting the CNN process the entire image.
This reduces the computation time for training and also increases the precision of the network. The process of how to select a location to focus on is called an \textbf{attention model} and is often implemented using a \textbf{recurrent neural network} (RNN). Besides image classification, attention models have also been highly successful for image captioning \cite{AttendAndTell} and machine translation \cite{machine_translation_attention}.

% Convolutional neural networks (CNNs) are often applied to image classification tasks but they are comparatively slow to train.
%face the problem of heavy computations 
%\cite{DeepMindAttention}. 
%Instead of attempting to process the entire image at once, we can mimic the human eye by


%Attention models have recently become popular for their ability to identify relevant information in complex pictures; successful examples include image classification \cite{DeepMindAttention}, machine translation \cite{machine_translation_attention} and image captioning \cite{AttendAndTell}. Attention models typically use Artificial Neural Networks, in particular Convolutional Neural Networks (CNNs), for encoding the image. The image can then be decoded by an RNN which has been trained to recognize which part of the image is relevant. The RNN chooses some action to perform depending on the task, such as generating joystick input to a video game or generating the next word for an image caption.

Another advantage of attention models is that the system can explicitly visualize which part of the image is being considered when performing an action \cite{AttendAndTell}. Thus it is easy for a human developer to see whether the system was looking at the relevant part(s) of the image when it made a mistake.


\section{Goals}

The goal of the thesis is to automatically extract high-level information for each photographed page of historical population records. The information under consideration is 1. page number, 2. year and 3. record type, where the record type is one of \{blank, birth, death, marriage\}. If time permits, extracting location (such as parish) will also be considered.

The system should learn from the currently indexed and waypointed data in a fully autonomous manner without feature engineering.

A perfect classification is unrealistic so the resulting groupings of images will need some manual inspection before publication. However, it should be sufficient to check some of the pages in a group instead of going through every page. Thus the manual workload should be significantly decreased. If the precision of the system is too low, correcting errors would be equally troublesome as doing the waypointing manually. Thus, the precision of the classification should be as high as possible.

%Although a perfect classification is unrealistic, the quality should be high enough that manual inspection and eventual correction should be considerably faster than waypointing the images manually.

%High precision is vital for usefulness since 
%A sufficiently high precision is needed so that checking the output of the system is faster than doing the waypointing manually.
%Although a prefect precision is unrealistic, ideally 


%A perfect precision is unrealistic so the generated output will need to be checked before publication. The quality of the output should be good enough so that correcting eventual mistakes is faster than doing the waypointing manually.

While maximizing the precision, the system should have a reasonable throughput. Processing millions of photographed pages should be viable.
%the system should be fast enough to be viable to process many millions of images.
%the processing time should be bounded to some r

\section{Challenges}

%Although we have simplified the original problem of indexing, 
Offline HWR is already considered a challenging problem because of the variations in writing style, both for the same scribe and for different scribes.
Additionally, the information we are interested in is written in various time periods on pages that are now partially degraded.
%We want to extract handwritten information from various time periods on pages that may be degraded.
Besides style variation, documents from different time periods and locations typically use different layouts, especially in the case of free-form writing.
%Furthermore, different documents structure the information in different ways so the system must be very adaptive, especially if one also considers records in free-form writing in addition to the printed templates.
%Furthermore, the information can be structured in many different ways, especially for records in free-form writing.
%Furthermore, if we consider free-form writing in addition to printed templates the relevant information can be structured in different ways.

%The information we want to extract is handwritten by different scribes in various structures on degraded pages. This variance and noise constitutes one of the greatest challenges towards automated information extraction.
%Thus, our system must be very noise-resistant.

Sometimes, year is not present on every page in a book. In this case, the indexer is instructed to type the year of the preceding page. This creates an incongruity between the image and the training label. The risk is that the system may learn to recognize noise instead of the actual written year. This could potentially be remedied by a sequential model that takes into account the classification of the previous image. Such a model could however introduce bias in the system.
A related problem is that multiple years can occur on the same page.

%Another challenge is that some images may not contain a page number, year or location at all although the indexed data suggests that it has.

%The system does not have to be perfect as the waypointing process will still require some human intervention. However, in order for the system to be useful it should have a sufficiently high precision to significantly reduce the human workload.

%Furthermore, the extracted information need to be (very) reliable in order to be useful.
%For the output to be useful, we also need to achieve a (very) high precision.

%Although the domains are limited, we still face the problem of multiple scribes and variation in page structure. Because some of the books are (very) old, their pages may somewhat degraded.

%\newpage

\section{Delimitations}

Although there are images in many languages from many places, the thesis will only consider images of text in a single language from a single country but over a longer period of time, possibly up to three hundred years.
However, the techniques applied should be general enough to handle many different languages, at least for extracting year and page number.


\section{Approach}

%It is generally difficult to find good training data for historical handwritten records in various languages. The manually extracted information could be considered for training although it is problematic to match the extracted information against the handwritten text in the image. Even if the image was automatically transcribed correctly, it is non-trivial to correctly identify names and locations as they can be confused with regular words, e.g. "Smith" or "Carpenter".

%Even for English handwritten text, current state-of-the-art handwriting recognition systems do not achieve a sufficiently high precision to be useful (TODO add numbers on current precision).


Ideally, waypointing should be solved by fully transcribing an image by HWR and then use \textbf{natural language processing} (NLP) techniques to understand the complete text. However, it is difficult to find good training data for historical handwritten records in various languages. The manually extracted information could be considered for training although it is problematic to match the extracted information against the handwritten text in the image. Even if the transcription worked correctly, we would also need to develop language models to understand the text which require even more training data. Finally, doing the required pre-processing of the image, HWR on the full text and then NLP on the result is computationally expensive and may not be feasible for a large volume of documents. Considering the example in Figure \ref{fig:example}, understanding the complete text might not be necessary to extract the relevant information in the record.

%However, there is currently not enough training data for a system to learn transcribing the full text.

Instead of HWR, we approach the problem from the perspective of image classification. We have relevant labels (year, page number, record type) for the images that have been indexed manually. For validation, we can also use images that have been waypointed but not yet indexed.

\subsection{Using deep neural networks with attention models}

As we have seen, deep neural networks with attention models are very successful at understanding images, both in image classification \cite{DeepMindAttention} and image captioning \cite{AttendAndTell}. The latter used CNNs, which are very good at identifying objects and shapes and have recently been suggested for offline HWR \cite{offline_HWR_CNN}.

Thus, a promising approach is using a CNN for automatic feature extraction together with an attention model implemented with an RNN to choose which parts of the image to focus on.
%As discussed previously, attention models are deep learning techniques that can make systems learn to find relevant elements in images autonomously.
%We will train a convolutional neural network (CNN) together with an attention model by extending previous work \cite{AttendAndTell, DeepMindAttention}.
We classify each image in three categories: page number, year and record type by using one softmax output layer per category. This means that years are represented as disjoint labels instead of actual numbers.
%to recognize  by building on previous work on attention models . Thus we will have a deep convolutional neural network (CNN) with one softmax layer per output. This means that we classify each year and page number as a different label rather than expressing it as a number.
Because we want the system to recognize digits well,
the CNN can be pre-trained on open digit recognition corpora like MNIST \cite{MNIST}.

We will also consider reducing training time by down-sampling and cropping the images.

%We will consider cropping and down-sampling of the images to reduce processing time.

% Often, page number, year and location can be found at the top of the page so we will consider processing the upper part of each image to speed up training. This is something we will experiment with.

The problem of having multiple years per page could be solved by using an RNN to generate a sequence of years for each image. This sequence would typically contain either a single year or two consecutive years.

\subsection{Prototype}

A prototype will be developed that performs waypointing using the popular machine learning framework Tensorflow \cite{Tensorflow}. Initially the prototype will extend existing open source code \cite{GithubAttendAndTell} but may be rewritten at a later point. The final prototype will be published as open source code.

\subsection{Evaluation}

Already indexed data from a single country will be used for training and validation; the data will possibly be restricted to some smaller region or time period.
The labeled images will be partitioned into $k$ disjoint sets.
Since the final system will work on new books that it didn't see before, we let all images from the same book belong to the same set, instead of randomly dividing the individual pages.
%All images from the same book should belong to the same set instead of .
% rather than new images from the same book.
The success of the project will be measured by the precision of the prototype by cross-validation over the $k$ sets.
% on currently indexed data 

%In order to see if the abstractions can truly be learned, the validation set should be a 

%In order for the validation to be realistic, the validation set should be a set of sequential pages, ideally written by a scribe that did not write anything for the training data. However, the validation data should be representative of the training data in terms of location and time period.

%In order for the evaluation to be truly representative, the validation data should contain images of pages written by a different scribe than the training data.

%Since this is the first automated waypointing system, we can only compare against human performance.


\subsection{Cooperation}

The thesis work will take place at Chalmers University of Technology in Gothenburg. Olof Mogren (PhD Student, \url{http://mogren.one}) has agreed to supervise the project if the program director will approve. We have discussed the problem in several meetings and agreed on the approach described above.

We have also been in contact with the Jon Morrey, director of the research department at FamilySearch, who has agreed to advise on the project. FamilySearch will supply the necessary data, i.e. photographs of historical records and their corresponding indexed information.




% \newpage